{
  "hash": "d5ab0212892ee8e2205b2b15739b2650",
  "result": {
    "markdown": "---\ntitle: \"Nonlinear Regression\"\nauthor: \"Tahmina Sultana Priya\"\ndate: \"2023-10-29\"\nimage: \"reg.png\"\ncategories: [news, code, analysis]\n---\n\n# What Is Nonlinear Regression? Comparison to Linear Regression\n\n![Nonlinear Regression](nlin.png){fig-alt=\"Nonlinear Regression\"}\n\nNonlinear regression is a statistical technique used to model data by expressing it as a mathematical function. Unlike simple linear regression, which uses a straight line to relate two variables (X and Y), nonlinear regression captures more complex, curved relationships between these variables.\n\nThe main objective of nonlinear regression is to minimize the sum of squared differences between the observed Y values and the predictions made by the nonlinear model. This sum of squares serves as a measure of how well the model fits the data points. To compute it, we calculate the differences between the fitted nonlinear function and each data point's Y value, square these differences, and then sum them up. A smaller sum of squared differences indicates a better fit of the model to the data.\n\nNonlinear regression employs various mathematical functions such as logarithmic, trigonometric, exponential, power functions, Lorenz curves, Gaussian functions, and other fitting techniques to capture the underlying relationships in the data.\n\n**Key Notes:**\n\n-   Linear and nonlinear regression are methods for predicting Y values based on an X variable (or multiple X variables).\n\n-   Nonlinear regression involves using a curved mathematical function of X variables to make predictions for a Y variable.\n\n-   Nonlinear regression can be employed to model and predict population growth trends over time.\n\nNonlinear regression modeling and linear regression modeling both aim to visually represent a specific response based on a set of variables. Nonlinear models are more intricate to construct compared to linear models because they involve approximations, often through trial-and-error iterations. Mathematicians employ established techniques like the Gauss-Newton method and the Levenberg-Marquardt method in this process.\n\nSometimes, what may initially appear as a nonlinear regression model can, in fact, be linear. To determine the underlying relationships in your data, you can use curve estimation procedures to choose the appropriate regression model, be it linear or nonlinear. Linear regression models, typically represented as straight lines, can also exhibit curves depending on the specific linear equation used. Additionally, it's possible to apply algebraic transformations to make a nonlinear equation resemble a linear one, termed as an \"intrinsically linear\" nonlinear equation.\n\n## Example of Nonlinear Regression \n\n**I.Introduction**\n\nIf the data shows a curvy trend, then linear regression will not produce\nvery accurate results when compared to a non-linear regression because,\nas the name implies, linear regression presumes that the data is\nlinear.\n\n**Importing required libraries**\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization\n```\n:::\n\n\n## 1. Linear\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nx = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 3*(x) + 2\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=596 height=429}\n:::\n:::\n\n\n## 2. Polynomial \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nx = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 1*(x**3) + 2*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=604 height=430}\n:::\n:::\n\n\n## 3. Quadratic\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nx = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\n\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=585 height=429}\n:::\n:::\n\n\n## 4. Exponential\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nX = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\n\nY= np.exp(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=593 height=429}\n:::\n:::\n\n\n## 5. Logarithmic \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nX = np.arange(1.0, 10.0, 0.1)\n\nY = np.log(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=589 height=429}\n:::\n:::\n\n\n## 6. Sigmoidal/Logistic\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nX = np.arange(-5.0, 5.0, 0.1)\n\n\nY = 1-4/(1+np.power(3, X-2))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=600 height=432}\n:::\n:::\n\n\n## Non-Linear Regression example with Dataset\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\npath = '../input/'\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/nlinregression/gdp.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nNumber of rows and columns in the data set:  (55, 2)\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1960</td>\n      <td>5.918412e+10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1961</td>\n      <td>4.955705e+10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1962</td>\n      <td>4.668518e+10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1963</td>\n      <td>5.009730e+10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1964</td>\n      <td>5.906225e+10</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nplt.figure(figsize=(8,5))\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=663 height=443}\n:::\n:::\n\n\n## \n\n**Choosing a model**\n\nFrom an initial look at the plot, we determine that the logistic\nfunction could be a good approximation, since it has the property of\nstarting with a slow growth, increasing growth in the middle, and then\ndecreasing again at the end; as illustrated below:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nX = np.arange(-5,5.0, 0.1)\nY = 1.0 / (1.0 + np.exp(-X))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=589 height=429}\n:::\n:::\n\n\n### Building The Model\n\nNow, let's build our regression model and initialize its parameters.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n   \n   \nbeta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=571 height=425}\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Lets normalize our data\nxdata =x_data/max(x_data)\nydata =y_data/max(y_data)\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n beta_1 = 690.451709, beta_2 = 0.997207\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nx = np.linspace(1960, 2015, 55)\nx = x/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=663 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# split data into train/test\nmsk = np.random.rand(len(df)) < 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean absolute error: 0.02\nResidual sum of squares (MSE): 0.00\nR2-score: 0.94\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}