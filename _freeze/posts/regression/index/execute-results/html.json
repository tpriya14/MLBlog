{
  "hash": "dd624310253a324780d43cd55fb31701",
  "result": {
    "markdown": "---\ntitle: \"Linear and nonlinear regression\"\nauthor: \"Tahmina Sultana Priya\"\ndate: \"2023-10-29\"\nimage: \"regression.jpg\"\ncategories: [news, code, analysis]\n---\n\n# Definition & Working principle\n\nLinear regression is a supervised learning technique employed when the\ntarget or dependent variable consists of continuous real numbers. It\naims to establish a connection between the dependent variable, denoted\nas *y* and one or multiple independent variables, typically represented\nas *x*, by determining the best-fit line. This approach operates based\non the principle of Ordinary Least Squares (OLS) or Mean Square Error\n(MSE). In statistical terms, OLS is a method used to estimate the\nunknown parameters of the linear regression function, with the objective\nof minimizing the sum of squared differences between the observed\ndependent variable in the given dataset and the values predicted by the\nlinear regression function.\n\n## Import Library and Dataset\n\n![](thumbnail.jpg)\n\nSince this post doesn't specify an explicit `image`, the first image in the post will be used in the listing page of posts.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import library\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Import dataset\n#path ='dataset/'\npath = '../input/'\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/regression/insurance.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nNumber of rows and columns in the data set:  (1338, 7)\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>bmi</th>\n      <th>children</th>\n      <th>smoker</th>\n      <th>region</th>\n      <th>charges</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>female</td>\n      <td>27.900</td>\n      <td>0</td>\n      <td>yes</td>\n      <td>southwest</td>\n      <td>16884.92400</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18</td>\n      <td>male</td>\n      <td>33.770</td>\n      <td>1</td>\n      <td>no</td>\n      <td>southeast</td>\n      <td>1725.55230</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28</td>\n      <td>male</td>\n      <td>33.000</td>\n      <td>3</td>\n      <td>no</td>\n      <td>southeast</td>\n      <td>4449.46200</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>33</td>\n      <td>male</td>\n      <td>22.705</td>\n      <td>0</td>\n      <td>no</td>\n      <td>northwest</td>\n      <td>21984.47061</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32</td>\n      <td>male</td>\n      <td>28.880</td>\n      <td>0</td>\n      <td>no</td>\n      <td>northwest</td>\n      <td>3866.85520</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n\"\"\" for our visualization purpose will fit line using seaborn library only for bmi as independent variable \nand charges as dependent variable\"\"\"\n\nsns.lmplot(x='bmi',y='charges',data=df,aspect=2,height=6)\nplt.xlabel('Boby Mass Index$(kg/m^2)$: as Independent variable')\nplt.ylabel('Insurance Charges: as Dependent variable')\nplt.title('Charge Vs BMI');\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=1142 height=591}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>bmi</th>\n      <th>children</th>\n      <th>charges</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1338.000000</td>\n      <td>1338.000000</td>\n      <td>1338.000000</td>\n      <td>1338.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>39.207025</td>\n      <td>30.663397</td>\n      <td>1.094918</td>\n      <td>13270.422265</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>14.049960</td>\n      <td>6.098187</td>\n      <td>1.205493</td>\n      <td>12110.011237</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>18.000000</td>\n      <td>15.960000</td>\n      <td>0.000000</td>\n      <td>1121.873900</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>27.000000</td>\n      <td>26.296250</td>\n      <td>0.000000</td>\n      <td>4740.287150</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>39.000000</td>\n      <td>30.400000</td>\n      <td>1.000000</td>\n      <td>9382.033000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>51.000000</td>\n      <td>34.693750</td>\n      <td>2.000000</td>\n      <td>16639.912515</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>64.000000</td>\n      <td>53.130000</td>\n      <td>5.000000</td>\n      <td>63770.428010</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Check for missing value\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nplt.figure(figsize=(12,4))\nsns.heatmap(df.isnull(),cbar=False,cmap='viridis',yticklabels=False)\nplt.title('Missing value in the dataset');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=912 height=357}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nf= plt.figure(figsize=(12,4))\n\nax=f.add_subplot(121)\nsns.histplot(df['charges'],bins=50,color='r',ax=ax)\nax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.histplot(np.log10(df['charges']),bins=40,color='b',ax=ax)\nax.set_title('Distribution of insurance charges in $log$ sacle')\nax.set_xscale('log');\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=965 height=377}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nf = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\nax.set_title('Violin plot of Charges vs sex')\n\nax = f.add_subplot(122)\nsns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\nax.set_title('Violin plot of Charges vs smoker');\n\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_294494/1103035780.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\n/tmp/ipykernel_294494/1103035780.py:7: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=1131 height=523}\n:::\n:::\n\n\nFrom left plot the insurance charge for male and female is approximatley\nin same range,it is average around 5000 bucks. In right plot the\ninsurance charge for smokers is much wide range compare to non smokers,\nthe average charges for non smoker is approximately 5000 bucks. For\nsmoker the minimum insurance charge is itself 5000 bucks.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nplt.figure(figsize=(14,6))\nsns.boxplot(x='children', y='charges',hue='sex',data=df,palette='rainbow')\nplt.title('Box plot of charges vs children');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=1131 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nplt.figure(figsize=(14,6))\nsns.violinplot(x='region', y='charges',hue='sex',data=df,palette='rainbow',split=True)\nplt.title('Violin plot of charges vs children');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=1131 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nf = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.scatterplot(x='age',y='charges',data=df,palette='magma',hue='smoker',ax=ax)\nax.set_title('Scatter plot of Charges vs age')\n\nax = f.add_subplot(122)\nsns.scatterplot(x='bmi',y='charges',data=df,palette='viridis',hue='smoker')\nax.set_title('Scatter plot of Charges vs bmi')\nplt.savefig('sc.png');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=1131 height=523}\n:::\n:::\n\n\n## Data Preprocessing\n\nData preprocessing in machine learning involves encoding categorical data into numerical form, as machine learning algorithms typically require numerical input. There are several techniques for this:\n\n1.  **Label Encoding:** This method involves converting categorical labels into numerical values to enable algorithms to work with them.\n\n2.  **One-Hot Encoding:** One-hot encoding represents categorical variables as binary vectors, making the data more expressive. First, the categorical values are mapped to integer values (label encoding), and then each integer is converted into a binary vector with all zeros except for the index of the integer, which is marked with a 1.\n\n3.  **Dummy Variable Trap:** This situation occurs when independent variables are multicollinear, meaning that two or more variables are highly correlated, making it possible to predict one variable from the others.\n\nTo simplify this process, the pandas library offers a convenient function called `get_dummies`. This function allows us to perform all three steps in a single line of code. We can use it to create dummy variables for features like 'sex,' 'children,' 'smoker,' and 'region.' By setting the `drop_first=True` parameter, we can automatically eliminate the dummy variable trap by dropping one variable and retaining the original variable. This makes data preprocessing more straightforward and efficient.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Dummy variable\ncategorical_columns = ['sex','children', 'smoker', 'region']\ndf_encode = pd.get_dummies(data = df, prefix = 'OHE', prefix_sep='_',\n               columns = categorical_columns,\n               drop_first =True,\n              dtype='int8')\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Lets verify the dummay variable process\nprint('Columns in original data frame:\\n',df.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df.shape)\nprint('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df_encode.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumns in original data frame:\n ['age' 'sex' 'bmi' 'children' 'smoker' 'region' 'charges']\n\nNumber of rows and columns in the dataset: (1338, 7)\n\nColumns in data frame after encoding dummy variable:\n ['age' 'bmi' 'charges' 'OHE_male' 'OHE_1' 'OHE_2' 'OHE_3' 'OHE_4' 'OHE_5'\n 'OHE_yes' 'OHE_northwest' 'OHE_southeast' 'OHE_southwest']\n\nNumber of rows and columns in the dataset: (1338, 13)\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\n\n#df['charges'] = y_bc  \n# it did not perform better for this model, so log transform is used\nci,lam\n## Log transform\ndf_encode['charges'] = np.log(df_encode['charges'])\n```\n:::\n\n\nThe original categorical variable are remove and also one of the one hot\nencode varible column for perticular categorical variable is droped\nfrom the column. So we completed all three encoding step by using get\ndummies function.\n\n## Train Test split\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nX = df_encode.drop('charges',axis=1) # Independet variable\ny = df_encode['charges'] # dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Step 1: add x0 =1 to dataset\nX_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\nX_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n\n# Step2: build model\ntheta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) \n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# The parameters for linear regression model\nparameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\ncolumns = ['intersect:x_0=1'] + list(X.columns.values)\nparameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Scikit Learn module\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.\n\n#Parameter\nsk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\nparameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\nparameter_df\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Parameter</th>\n      <th>Columns</th>\n      <th>theta</th>\n      <th>Sklearn_theta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>theta_0</td>\n      <td>intersect:x_0=1</td>\n      <td>7.059171</td>\n      <td>7.059171</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>theta_1</td>\n      <td>age</td>\n      <td>0.033134</td>\n      <td>0.033134</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>theta_2</td>\n      <td>bmi</td>\n      <td>0.013517</td>\n      <td>0.013517</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>theta_3</td>\n      <td>OHE_male</td>\n      <td>-0.067767</td>\n      <td>-0.067767</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>theta_4</td>\n      <td>OHE_1</td>\n      <td>0.149457</td>\n      <td>0.149457</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>theta_5</td>\n      <td>OHE_2</td>\n      <td>0.272919</td>\n      <td>0.272919</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>theta_6</td>\n      <td>OHE_3</td>\n      <td>0.244095</td>\n      <td>0.244095</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>theta_7</td>\n      <td>OHE_4</td>\n      <td>0.523339</td>\n      <td>0.523339</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>theta_8</td>\n      <td>OHE_5</td>\n      <td>0.466030</td>\n      <td>0.466030</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>theta_9</td>\n      <td>OHE_yes</td>\n      <td>1.550481</td>\n      <td>1.550481</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>theta_10</td>\n      <td>OHE_northwest</td>\n      <td>-0.055845</td>\n      <td>-0.055845</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>theta_11</td>\n      <td>OHE_southeast</td>\n      <td>-0.146578</td>\n      <td>-0.146578</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>theta_12</td>\n      <td>OHE_southwest</td>\n      <td>-0.133508</td>\n      <td>-0.133508</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe parameter obtained from both the model are same.So we succefull\nbuild our model using normal equation and verified using sklearn linear\nregression module. Let's move ahead, next step is prediction and model\nevaluvation.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Normal equation\ny_pred_norm =  np.matmul(X_test_0,theta)\n\n#Evaluvation: MSE\nJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n\n# R_square \nsse = np.sum((y_pred_norm - y_test)**2)\nsst = np.sum((y_test - y_test.mean())**2)\nR_square = 1 - (sse/sst)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\nprint('R square obtain for normal equation method is :',R_square)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Mean Square Error(MSE) or J(theta) is:  0.18729622322982042\nR square obtain for normal equation method is : 0.7795687545055301\n```\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# sklearn regression module\ny_pred_sk = lin_reg.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk = mean_squared_error(y_pred_sk, y_test)\n\n# R_square\nR_square_sk = lin_reg.score(X_test,y_test)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\nprint('R square obtain for scikit learn library is :',R_square_sk)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Mean Square Error(MSE) or J(theta) is:  0.1872962232298189\nR square obtain for scikit learn library is : 0.7795687545055319\n```\n:::\n:::\n\n\nModel validation is a crucial step in assessing the performance of a linear regression model, and it involves checking various assumptions. The key assumptions for a linear regression model are as follows:\n\n1.  **Linear Relationship:** Linear regression assumes that the relationship between the dependent and independent variables is linear. You can verify this assumption by creating a scatter plot of actual values against predicted values.\n\n2.  **Normality of Residuals:** The residual errors should follow a normal distribution. This can be checked by examining the distribution of the residuals.\n\n3.  **Mean of Residuals:** The mean of the residual errors should ideally be close to 0.\n\n4.  **Multivariate Normality:** Linear regression assumes that all variables are multivariate normally distributed. This assumption can be assessed using a Q-Q plot.\n\n5.  **Multicollinearity:** Linear regression assumes minimal multicollinearity, meaning that independent variables are not highly correlated with each other. The variance inflation factor (VIF) can help identify and measure the strength of such correlations. A VIF greater than 1 but less than 5 indicates moderate correlation, while a VIF less than 5 suggests a critical level of multicollinearity.\n\n6.  **Homoscedasticity:** The data should exhibit homoscedasticity, which means that the residuals are roughly equal across the regression line. You can assess this by creating a scatter plot of residuals against the fitted values. If the plot shows a funnel-shaped pattern, it indicates heteroscedasticity.\n\nEnsuring these assumptions are met is essential to build a reliable linear regression model.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# Check for Linearity\nf = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\n#sns.scatterplot(data=df, y_test, y_pred_sk)\nsns.scatterplot(x=y_test,y=y_pred_sk,ax=ax,color='r')\nax.set_title('Check for Linearity:\\n Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\nsns.histplot((y_test - y_pred_sk),ax=ax,color='b')\nax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-1.png){width=1100 height=467}\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# Check for Multivariate Normality\n# Quantile-Quantile plot \nf,ax = plt.subplots(1,2,figsize=(14,6))\nimport scipy as sp\n_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])\nax[0].set_title('Check for Multivariate Normality: \\nQ-Q Plot')\n\n#Check for Homoscedasticity\nsns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') \nax[1].set_title('Check for Homoscedasticity: \\nResidual Vs Predicted');\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-1.png){width=1121 height=541}\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# Check for Multicollinearity\n#Variance Inflation Factor\nVIF = 1/(1- R_square_sk)\nVIF\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n4.536561945911138\n```\n:::\n:::\n\n\nHere are the model assumptions for linear regression, along with their assessment:\n\n1.  The actual vs. predicted plot doesn't form a linear pattern, indicating a failure of the linear assumption.\n\n2.  The mean of the residuals is close to zero, and the residual error plot is skewed to the right.\n\n3.  The Q-Q plot shows that values greater than 1.5 tend to increase, suggesting a departure from multivariate normality.\n\n4.  The plot exhibits heteroscedasticity, with errors increasing after a certain point.\n\n5.  The variance inflation factor is less than 5, indicating the absence of multicollinearity.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}