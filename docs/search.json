[
  {
    "objectID": "posts/linregression/index.html",
    "href": "posts/linregression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is a supervised learning technique employed when the target or dependent variable consists of continuous real numbers. It aims to establish a connection between the dependent variable, denoted as y and one or multiple independent variables, typically represented as x, by determining the best-fit line. This approach operates based on the principle of Ordinary Least Squares (OLS) or Mean Square Error (MSE). In statistical terms, OLS is a method used to estimate the unknown parameters of the linear regression function, with the objective of minimizing the sum of squared differences between the observed dependent variable in the given dataset and the values predicted by the linear regression function.\n\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n# Import library\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization\n\n\n# Import dataset\n#path ='dataset/'\npath = '../input/'\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/linregression/insurance.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n\n\nNumber of rows and columns in the data set:  (1338, 7)\n\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n\n\n\n\n\n\n\"\"\" for our visualization purpose will fit line using seaborn library only for bmi as independent variable \nand charges as dependent variable\"\"\"\n\nsns.lmplot(x='bmi',y='charges',data=df,aspect=2,height=6)\nplt.xlabel('Boby Mass Index$(kg/m^2)$: as Independent variable')\nplt.ylabel('Insurance Charges: as Dependent variable')\nplt.title('Charge Vs BMI');\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\ncharges\n\n\n\n\ncount\n1338.000000\n1338.000000\n1338.000000\n1338.000000\n\n\nmean\n39.207025\n30.663397\n1.094918\n13270.422265\n\n\nstd\n14.049960\n6.098187\n1.205493\n12110.011237\n\n\nmin\n18.000000\n15.960000\n0.000000\n1121.873900\n\n\n25%\n27.000000\n26.296250\n0.000000\n4740.287150\n\n\n50%\n39.000000\n30.400000\n1.000000\n9382.033000\n\n\n75%\n51.000000\n34.693750\n2.000000\n16639.912515\n\n\nmax\n64.000000\n53.130000\n5.000000\n63770.428010\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12,4))\nsns.heatmap(df.isnull(),cbar=False,cmap='viridis',yticklabels=False)\nplt.title('Missing value in the dataset');\n\n\n\n\n\nf= plt.figure(figsize=(12,4))\n\nax=f.add_subplot(121)\nsns.histplot(df['charges'],bins=50,color='r',ax=ax)\nax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.histplot(np.log10(df['charges']),bins=40,color='b',ax=ax)\nax.set_title('Distribution of insurance charges in $log$ sacle')\nax.set_xscale('log');\n\n\n\n\n\nf = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\nax.set_title('Violin plot of Charges vs sex')\n\nax = f.add_subplot(122)\nsns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\nax.set_title('Violin plot of Charges vs smoker');\n\n/tmp/ipykernel_49564/1103035780.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\n/tmp/ipykernel_49564/1103035780.py:7: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\n\n\n\n\n\nFrom left plot the insurance charge for male and female is approximatley in same range,it is average around 5000 bucks. In right plot the insurance charge for smokers is much wide range compare to non smokers, the average charges for non smoker is approximately 5000 bucks. For smoker the minimum insurance charge is itself 5000 bucks.\n\nplt.figure(figsize=(14,6))\nsns.boxplot(x='children', y='charges',hue='sex',data=df,palette='rainbow')\nplt.title('Box plot of charges vs children');\n\n\n\n\n\nplt.figure(figsize=(14,6))\nsns.violinplot(x='region', y='charges',hue='sex',data=df,palette='rainbow',split=True)\nplt.title('Violin plot of charges vs children');\n\n\n\n\n\nf = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.scatterplot(x='age',y='charges',data=df,palette='magma',hue='smoker',ax=ax)\nax.set_title('Scatter plot of Charges vs age')\n\nax = f.add_subplot(122)\nsns.scatterplot(x='bmi',y='charges',data=df,palette='viridis',hue='smoker')\nax.set_title('Scatter plot of Charges vs bmi')\nplt.savefig('sc.png');\n\n\n\n\n\n\n\n\nData preprocessing in machine learning involves encoding categorical data into numerical form, as machine learning algorithms typically require numerical input. There are several techniques for this:\n\nLabel Encoding: This method involves converting categorical labels into numerical values to enable algorithms to work with them.\nOne-Hot Encoding: One-hot encoding represents categorical variables as binary vectors, making the data more expressive. First, the categorical values are mapped to integer values (label encoding), and then each integer is converted into a binary vector with all zeros except for the index of the integer, which is marked with a 1.\nDummy Variable Trap: This situation occurs when independent variables are multicollinear, meaning that two or more variables are highly correlated, making it possible to predict one variable from the others.\n\nTo simplify this process, the pandas library offers a convenient function called get_dummies. This function allows us to perform all three steps in a single line of code. We can use it to create dummy variables for features like ‘sex,’ ‘children,’ ‘smoker,’ and ‘region.’ By setting the drop_first=True parameter, we can automatically eliminate the dummy variable trap by dropping one variable and retaining the original variable. This makes data preprocessing more straightforward and efficient.\n\n# Dummy variable\ncategorical_columns = ['sex','children', 'smoker', 'region']\ndf_encode = pd.get_dummies(data = df, prefix = 'OHE', prefix_sep='_',\n               columns = categorical_columns,\n               drop_first =True,\n              dtype='int8')\n\n\n# Lets verify the dummay variable process\nprint('Columns in original data frame:\\n',df.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df.shape)\nprint('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df_encode.shape)\n\nColumns in original data frame:\n ['age' 'sex' 'bmi' 'children' 'smoker' 'region' 'charges']\n\nNumber of rows and columns in the dataset: (1338, 7)\n\nColumns in data frame after encoding dummy variable:\n ['age' 'bmi' 'charges' 'OHE_male' 'OHE_1' 'OHE_2' 'OHE_3' 'OHE_4' 'OHE_5'\n 'OHE_yes' 'OHE_northwest' 'OHE_southeast' 'OHE_southwest']\n\nNumber of rows and columns in the dataset: (1338, 13)\n\n\n\nfrom scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\n\n#df['charges'] = y_bc  \n# it did not perform better for this model, so log transform is used\nci,lam\n## Log transform\ndf_encode['charges'] = np.log(df_encode['charges'])\n\nThe original categorical variable are remove and also one of the one hot encode varible column for perticular categorical variable is droped from the column. So we completed all three encoding step by using get dummies function.\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX = df_encode.drop('charges',axis=1) # Independet variable\ny = df_encode['charges'] # dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)\n\n\n# Step 1: add x0 =1 to dataset\nX_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\nX_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n\n# Step2: build model\ntheta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) \n\n\n# The parameters for linear regression model\nparameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\ncolumns = ['intersect:x_0=1'] + list(X.columns.values)\nparameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})\n\n\n# Scikit Learn module\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.\n\n#Parameter\nsk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\nparameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\nparameter_df\n\n\n\n\n\n\n\n\nParameter\nColumns\ntheta\nSklearn_theta\n\n\n\n\n0\ntheta_0\nintersect:x_0=1\n7.059171\n7.059171\n\n\n1\ntheta_1\nage\n0.033134\n0.033134\n\n\n2\ntheta_2\nbmi\n0.013517\n0.013517\n\n\n3\ntheta_3\nOHE_male\n-0.067767\n-0.067767\n\n\n4\ntheta_4\nOHE_1\n0.149457\n0.149457\n\n\n5\ntheta_5\nOHE_2\n0.272919\n0.272919\n\n\n6\ntheta_6\nOHE_3\n0.244095\n0.244095\n\n\n7\ntheta_7\nOHE_4\n0.523339\n0.523339\n\n\n8\ntheta_8\nOHE_5\n0.466030\n0.466030\n\n\n9\ntheta_9\nOHE_yes\n1.550481\n1.550481\n\n\n10\ntheta_10\nOHE_northwest\n-0.055845\n-0.055845\n\n\n11\ntheta_11\nOHE_southeast\n-0.146578\n-0.146578\n\n\n12\ntheta_12\nOHE_southwest\n-0.133508\n-0.133508\n\n\n\n\n\n\n\nThe parameter obtained from both the model are same.So we succefull build our model using normal equation and verified using sklearn linear regression module. Let’s move ahead, next step is prediction and model evaluvation.\n\n# Normal equation\ny_pred_norm =  np.matmul(X_test_0,theta)\n\n#Evaluvation: MSE\nJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n\n# R_square \nsse = np.sum((y_pred_norm - y_test)**2)\nsst = np.sum((y_test - y_test.mean())**2)\nR_square = 1 - (sse/sst)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\nprint('R square obtain for normal equation method is :',R_square)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.18729622322982042\nR square obtain for normal equation method is : 0.7795687545055301\n\n\n\n# sklearn regression module\ny_pred_sk = lin_reg.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk = mean_squared_error(y_pred_sk, y_test)\n\n# R_square\nR_square_sk = lin_reg.score(X_test,y_test)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\nprint('R square obtain for scikit learn library is :',R_square_sk)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.1872962232298189\nR square obtain for scikit learn library is : 0.7795687545055319\n\n\nModel validation is a crucial step in assessing the performance of a linear regression model, and it involves checking various assumptions. The key assumptions for a linear regression model are as follows:\n\nLinear Relationship: Linear regression assumes that the relationship between the dependent and independent variables is linear. You can verify this assumption by creating a scatter plot of actual values against predicted values.\nNormality of Residuals: The residual errors should follow a normal distribution. This can be checked by examining the distribution of the residuals.\nMean of Residuals: The mean of the residual errors should ideally be close to 0.\nMultivariate Normality: Linear regression assumes that all variables are multivariate normally distributed. This assumption can be assessed using a Q-Q plot.\nMulticollinearity: Linear regression assumes minimal multicollinearity, meaning that independent variables are not highly correlated with each other. The variance inflation factor (VIF) can help identify and measure the strength of such correlations. A VIF greater than 1 but less than 5 indicates moderate correlation, while a VIF less than 5 suggests a critical level of multicollinearity.\nHomoscedasticity: The data should exhibit homoscedasticity, which means that the residuals are roughly equal across the regression line. You can assess this by creating a scatter plot of residuals against the fitted values. If the plot shows a funnel-shaped pattern, it indicates heteroscedasticity.\n\nEnsuring these assumptions are met is essential to build a reliable linear regression model.\n\n# Check for Linearity\nf = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\n#sns.scatterplot(data=df, y_test, y_pred_sk)\nsns.scatterplot(x=y_test,y=y_pred_sk,ax=ax,color='r')\nax.set_title('Check for Linearity:\\n Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\nsns.histplot((y_test - y_pred_sk),ax=ax,color='b')\nax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror');\n\n\n\n\n\n# Check for Multivariate Normality\n# Quantile-Quantile plot \nf,ax = plt.subplots(1,2,figsize=(14,6))\nimport scipy as sp\n_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])\nax[0].set_title('Check for Multivariate Normality: \\nQ-Q Plot')\n\n#Check for Homoscedasticity\nsns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') \nax[1].set_title('Check for Homoscedasticity: \\nResidual Vs Predicted');\n\n\n\n\n\n# Check for Multicollinearity\n#Variance Inflation Factor\nVIF = 1/(1- R_square_sk)\nVIF\n\n4.536561945911138\n\n\nHere are the model assumptions for linear regression, along with their assessment:\n\nThe actual vs. predicted plot doesn’t form a linear pattern, indicating a failure of the linear assumption.\nThe mean of the residuals is close to zero, and the residual error plot is skewed to the right.\nThe Q-Q plot shows that values greater than 1.5 tend to increase, suggesting a departure from multivariate normality.\nThe plot exhibits heteroscedasticity, with errors increasing after a certain point.\nThe variance inflation factor is less than 5, indicating the absence of multicollinearity."
  },
  {
    "objectID": "posts/linregression/index.html#import-library-and-dataset",
    "href": "posts/linregression/index.html#import-library-and-dataset",
    "title": "Linear Regression",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n# Import library\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization\n\n\n# Import dataset\n#path ='dataset/'\npath = '../input/'\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/linregression/insurance.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n\n\nNumber of rows and columns in the data set:  (1338, 7)\n\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n\n\n\n\n\n\n\"\"\" for our visualization purpose will fit line using seaborn library only for bmi as independent variable \nand charges as dependent variable\"\"\"\n\nsns.lmplot(x='bmi',y='charges',data=df,aspect=2,height=6)\nplt.xlabel('Boby Mass Index$(kg/m^2)$: as Independent variable')\nplt.ylabel('Insurance Charges: as Dependent variable')\nplt.title('Charge Vs BMI');\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\ncharges\n\n\n\n\ncount\n1338.000000\n1338.000000\n1338.000000\n1338.000000\n\n\nmean\n39.207025\n30.663397\n1.094918\n13270.422265\n\n\nstd\n14.049960\n6.098187\n1.205493\n12110.011237\n\n\nmin\n18.000000\n15.960000\n0.000000\n1121.873900\n\n\n25%\n27.000000\n26.296250\n0.000000\n4740.287150\n\n\n50%\n39.000000\n30.400000\n1.000000\n9382.033000\n\n\n75%\n51.000000\n34.693750\n2.000000\n16639.912515\n\n\nmax\n64.000000\n53.130000\n5.000000\n63770.428010\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(12,4))\nsns.heatmap(df.isnull(),cbar=False,cmap='viridis',yticklabels=False)\nplt.title('Missing value in the dataset');\n\n\n\n\n\nf= plt.figure(figsize=(12,4))\n\nax=f.add_subplot(121)\nsns.histplot(df['charges'],bins=50,color='r',ax=ax)\nax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.histplot(np.log10(df['charges']),bins=40,color='b',ax=ax)\nax.set_title('Distribution of insurance charges in $log$ sacle')\nax.set_xscale('log');\n\n\n\n\n\nf = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\nax.set_title('Violin plot of Charges vs sex')\n\nax = f.add_subplot(122)\nsns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\nax.set_title('Violin plot of Charges vs smoker');\n\n/tmp/ipykernel_49564/1103035780.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\n/tmp/ipykernel_49564/1103035780.py:7: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\n\n\n\n\n\nFrom left plot the insurance charge for male and female is approximatley in same range,it is average around 5000 bucks. In right plot the insurance charge for smokers is much wide range compare to non smokers, the average charges for non smoker is approximately 5000 bucks. For smoker the minimum insurance charge is itself 5000 bucks.\n\nplt.figure(figsize=(14,6))\nsns.boxplot(x='children', y='charges',hue='sex',data=df,palette='rainbow')\nplt.title('Box plot of charges vs children');\n\n\n\n\n\nplt.figure(figsize=(14,6))\nsns.violinplot(x='region', y='charges',hue='sex',data=df,palette='rainbow',split=True)\nplt.title('Violin plot of charges vs children');\n\n\n\n\n\nf = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.scatterplot(x='age',y='charges',data=df,palette='magma',hue='smoker',ax=ax)\nax.set_title('Scatter plot of Charges vs age')\n\nax = f.add_subplot(122)\nsns.scatterplot(x='bmi',y='charges',data=df,palette='viridis',hue='smoker')\nax.set_title('Scatter plot of Charges vs bmi')\nplt.savefig('sc.png');"
  },
  {
    "objectID": "posts/linregression/index.html#data-preprocessing",
    "href": "posts/linregression/index.html#data-preprocessing",
    "title": "Linear Regression",
    "section": "",
    "text": "Data preprocessing in machine learning involves encoding categorical data into numerical form, as machine learning algorithms typically require numerical input. There are several techniques for this:\n\nLabel Encoding: This method involves converting categorical labels into numerical values to enable algorithms to work with them.\nOne-Hot Encoding: One-hot encoding represents categorical variables as binary vectors, making the data more expressive. First, the categorical values are mapped to integer values (label encoding), and then each integer is converted into a binary vector with all zeros except for the index of the integer, which is marked with a 1.\nDummy Variable Trap: This situation occurs when independent variables are multicollinear, meaning that two or more variables are highly correlated, making it possible to predict one variable from the others.\n\nTo simplify this process, the pandas library offers a convenient function called get_dummies. This function allows us to perform all three steps in a single line of code. We can use it to create dummy variables for features like ‘sex,’ ‘children,’ ‘smoker,’ and ‘region.’ By setting the drop_first=True parameter, we can automatically eliminate the dummy variable trap by dropping one variable and retaining the original variable. This makes data preprocessing more straightforward and efficient.\n\n# Dummy variable\ncategorical_columns = ['sex','children', 'smoker', 'region']\ndf_encode = pd.get_dummies(data = df, prefix = 'OHE', prefix_sep='_',\n               columns = categorical_columns,\n               drop_first =True,\n              dtype='int8')\n\n\n# Lets verify the dummay variable process\nprint('Columns in original data frame:\\n',df.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df.shape)\nprint('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df_encode.shape)\n\nColumns in original data frame:\n ['age' 'sex' 'bmi' 'children' 'smoker' 'region' 'charges']\n\nNumber of rows and columns in the dataset: (1338, 7)\n\nColumns in data frame after encoding dummy variable:\n ['age' 'bmi' 'charges' 'OHE_male' 'OHE_1' 'OHE_2' 'OHE_3' 'OHE_4' 'OHE_5'\n 'OHE_yes' 'OHE_northwest' 'OHE_southeast' 'OHE_southwest']\n\nNumber of rows and columns in the dataset: (1338, 13)\n\n\n\nfrom scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\n\n#df['charges'] = y_bc  \n# it did not perform better for this model, so log transform is used\nci,lam\n## Log transform\ndf_encode['charges'] = np.log(df_encode['charges'])\n\nThe original categorical variable are remove and also one of the one hot encode varible column for perticular categorical variable is droped from the column. So we completed all three encoding step by using get dummies function."
  },
  {
    "objectID": "posts/linregression/index.html#train-test-split",
    "href": "posts/linregression/index.html#train-test-split",
    "title": "Linear Regression",
    "section": "",
    "text": "from sklearn.model_selection import train_test_split\nX = df_encode.drop('charges',axis=1) # Independet variable\ny = df_encode['charges'] # dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)\n\n\n# Step 1: add x0 =1 to dataset\nX_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\nX_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n\n# Step2: build model\ntheta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) \n\n\n# The parameters for linear regression model\nparameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\ncolumns = ['intersect:x_0=1'] + list(X.columns.values)\nparameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})\n\n\n# Scikit Learn module\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.\n\n#Parameter\nsk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\nparameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\nparameter_df\n\n\n\n\n\n\n\n\nParameter\nColumns\ntheta\nSklearn_theta\n\n\n\n\n0\ntheta_0\nintersect:x_0=1\n7.059171\n7.059171\n\n\n1\ntheta_1\nage\n0.033134\n0.033134\n\n\n2\ntheta_2\nbmi\n0.013517\n0.013517\n\n\n3\ntheta_3\nOHE_male\n-0.067767\n-0.067767\n\n\n4\ntheta_4\nOHE_1\n0.149457\n0.149457\n\n\n5\ntheta_5\nOHE_2\n0.272919\n0.272919\n\n\n6\ntheta_6\nOHE_3\n0.244095\n0.244095\n\n\n7\ntheta_7\nOHE_4\n0.523339\n0.523339\n\n\n8\ntheta_8\nOHE_5\n0.466030\n0.466030\n\n\n9\ntheta_9\nOHE_yes\n1.550481\n1.550481\n\n\n10\ntheta_10\nOHE_northwest\n-0.055845\n-0.055845\n\n\n11\ntheta_11\nOHE_southeast\n-0.146578\n-0.146578\n\n\n12\ntheta_12\nOHE_southwest\n-0.133508\n-0.133508\n\n\n\n\n\n\n\nThe parameter obtained from both the model are same.So we succefull build our model using normal equation and verified using sklearn linear regression module. Let’s move ahead, next step is prediction and model evaluvation.\n\n# Normal equation\ny_pred_norm =  np.matmul(X_test_0,theta)\n\n#Evaluvation: MSE\nJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n\n# R_square \nsse = np.sum((y_pred_norm - y_test)**2)\nsst = np.sum((y_test - y_test.mean())**2)\nR_square = 1 - (sse/sst)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\nprint('R square obtain for normal equation method is :',R_square)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.18729622322982042\nR square obtain for normal equation method is : 0.7795687545055301\n\n\n\n# sklearn regression module\ny_pred_sk = lin_reg.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk = mean_squared_error(y_pred_sk, y_test)\n\n# R_square\nR_square_sk = lin_reg.score(X_test,y_test)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\nprint('R square obtain for scikit learn library is :',R_square_sk)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.1872962232298189\nR square obtain for scikit learn library is : 0.7795687545055319\n\n\nModel validation is a crucial step in assessing the performance of a linear regression model, and it involves checking various assumptions. The key assumptions for a linear regression model are as follows:\n\nLinear Relationship: Linear regression assumes that the relationship between the dependent and independent variables is linear. You can verify this assumption by creating a scatter plot of actual values against predicted values.\nNormality of Residuals: The residual errors should follow a normal distribution. This can be checked by examining the distribution of the residuals.\nMean of Residuals: The mean of the residual errors should ideally be close to 0.\nMultivariate Normality: Linear regression assumes that all variables are multivariate normally distributed. This assumption can be assessed using a Q-Q plot.\nMulticollinearity: Linear regression assumes minimal multicollinearity, meaning that independent variables are not highly correlated with each other. The variance inflation factor (VIF) can help identify and measure the strength of such correlations. A VIF greater than 1 but less than 5 indicates moderate correlation, while a VIF less than 5 suggests a critical level of multicollinearity.\nHomoscedasticity: The data should exhibit homoscedasticity, which means that the residuals are roughly equal across the regression line. You can assess this by creating a scatter plot of residuals against the fitted values. If the plot shows a funnel-shaped pattern, it indicates heteroscedasticity.\n\nEnsuring these assumptions are met is essential to build a reliable linear regression model.\n\n# Check for Linearity\nf = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\n#sns.scatterplot(data=df, y_test, y_pred_sk)\nsns.scatterplot(x=y_test,y=y_pred_sk,ax=ax,color='r')\nax.set_title('Check for Linearity:\\n Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\nsns.histplot((y_test - y_pred_sk),ax=ax,color='b')\nax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror');\n\n\n\n\n\n# Check for Multivariate Normality\n# Quantile-Quantile plot \nf,ax = plt.subplots(1,2,figsize=(14,6))\nimport scipy as sp\n_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])\nax[0].set_title('Check for Multivariate Normality: \\nQ-Q Plot')\n\n#Check for Homoscedasticity\nsns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') \nax[1].set_title('Check for Homoscedasticity: \\nResidual Vs Predicted');\n\n\n\n\n\n# Check for Multicollinearity\n#Variance Inflation Factor\nVIF = 1/(1- R_square_sk)\nVIF\n\n4.536561945911138\n\n\nHere are the model assumptions for linear regression, along with their assessment:\n\nThe actual vs. predicted plot doesn’t form a linear pattern, indicating a failure of the linear assumption.\nThe mean of the residuals is close to zero, and the residual error plot is skewed to the right.\nThe Q-Q plot shows that values greater than 1.5 tend to increase, suggesting a departure from multivariate normality.\nThe plot exhibits heteroscedasticity, with errors increasing after a certain point.\nThe variance inflation factor is less than 5, indicating the absence of multicollinearity."
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Machine Learning Algorithms for Classification (src: https://towardsdatascience.com/top-machine-learning-algorithms-for-classification-2197870ff501)"
  },
  {
    "objectID": "posts/classification/index.html#introduction-to-classification",
    "href": "posts/classification/index.html#introduction-to-classification",
    "title": "Classification",
    "section": "Introduction to Classification",
    "text": "Introduction to Classification\nIn the dynamic world of machine learning, classification stands out as a pivotal concept, providing the ability to categorize and interpret data for a wide array of applications. Whether you’re looking to filter spam emails, identify diseases from medical data, or recognize handwritten digits, classification algorithms are your go-to tools. In this blog post, we’ll delve into what classification is in machine learning, its significance, and how it’s transforming industries."
  },
  {
    "objectID": "posts/classification/index.html#defining-classification-in-machine-learning",
    "href": "posts/classification/index.html#defining-classification-in-machine-learning",
    "title": "Classification",
    "section": "Defining Classification in Machine Learning",
    "text": "Defining Classification in Machine Learning\nAt its core, classification is the process of recognizing, understanding, and grouping data into predefined categories or subgroups. It’s like having a smart assistant that can look at a new piece of data and tell you which group it belongs to based on its characteristics. This task is accomplished through the analysis of historical data, which serves as a training ground for machine learning models."
  },
  {
    "objectID": "posts/classification/index.html#the-power-of-predefined-categories",
    "href": "posts/classification/index.html#the-power-of-predefined-categories",
    "title": "Classification",
    "section": "The Power of Predefined Categories",
    "text": "The Power of Predefined Categories\nThe magic of classification lies in these predefined categories. Think of them as labels, such as “spam” and “non-spam” for emails, “fraudulent” and “legitimate” for financial transactions, or “cat” and “dog” for image recognition. The ability to organize data into these categories enables decision-making, automation, and insights that would otherwise be impractical or impossible to achieve manually."
  },
  {
    "objectID": "posts/classification/index.html#how-classification-works",
    "href": "posts/classification/index.html#how-classification-works",
    "title": "Classification",
    "section": "How Classification Works",
    "text": "How Classification Works\nTo perform classification, machine learning algorithms need to learn from data first. This “training” phase involves feeding the algorithm a labeled dataset, where each data point is associated with the category it belongs to. The algorithm then learns the patterns, relationships, and features that characterize each category.\nOnce trained, the algorithm can classify new, unseen data by assessing its similarity to the patterns it has learned. It predicts the likelihood of the new data point falling into one of the predefined categories. This process is akin to your email provider recognizing whether an incoming email is spam or not based on past experiences."
  },
  {
    "objectID": "posts/classification/index.html#real-life-applications",
    "href": "posts/classification/index.html#real-life-applications",
    "title": "Classification",
    "section": "Real-Life Applications",
    "text": "Real-Life Applications\nClassification has found its way into countless real-world applications. From medical diagnoses to recommendation systems, here are a few examples:\n\nMedical Diagnoses: Doctors use machine learning models to predict whether a patient has a particular disease based on symptoms, medical history, and test results.\nRecommendation Systems: Companies like Netflix and Amazon employ classification to recommend movies or products to users based on their preferences and behavior.\nSentiment Analysis: Social media platforms analyze posts to classify them as positive, negative, or neutral, providing valuable insights into public opinion.\nImage Recognition: In the field of computer vision, classification helps identify objects, animals, or handwritten text in images.\n\n\nPopular Classification Algorithms:\n\nLogistic Regression: Logistic regression is a widely used classification algorithm that models the probability of an input belonging to a particular category. It’s simple, interpretable, and effective for binary and multiclass classification tasks.\nNaive Bayes: Naive Bayes is a probabilistic classification algorithm based on Bayes’ theorem. It’s particularly suited for text classification tasks and spam email filtering, where it assumes independence between features.\nK-Nearest Neighbors: K-NN is a straightforward yet powerful algorithm that classifies data points based on the majority class among their k-nearest neighbors. It’s versatile and can be applied to various types of data, but the choice of k is crucial for its performance.\nDecision Tree: Decision tree classifiers make decisions by splitting data based on features, creating a tree-like structure of decisions. They are interpretable and can handle both categorical and numerical data, making them useful in many applications.\nSupport Vector Machines: SVMs are effective for both linear and nonlinear classification tasks. They work by finding the optimal hyperplane that maximizes the margin between classes, making them robust against overfitting and suitable for high-dimensional data."
  },
  {
    "objectID": "posts/classification/index.html#example-heart-disease-prediction",
    "href": "posts/classification/index.html#example-heart-disease-prediction",
    "title": "Classification",
    "section": "Example: Heart Disease Prediction",
    "text": "Example: Heart Disease Prediction\n\nimport pandas as pd \nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import roc_curve, precision_recall_curve\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, f1_score, accuracy_score\nfrom sklearn.metrics import make_scorer, precision_score, precision_recall_curve\nfrom sklearn.metrics import recall_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nsns.set_theme(context='notebook',\n              style='white',\n              palette='deep',\n              font_scale=1.5,\n              color_codes=True,\n              rc=None)\n\nimport matplotlib\n\nplt.rcParams['figure.figsize'] = (14,8) \nplt.rcParams['figure.facecolor'] = '#F0F8FF'\nplt.rcParams['figure.titlesize'] = 'medium'\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['figure.edgecolor'] = 'green'\nplt.rcParams['figure.frameon'] = True\n\nplt.rcParams[\"figure.autolayout\"] = True\n\nplt.rcParams['axes.facecolor'] = '#F5F5DC'\nplt.rcParams['axes.titlesize'] = 25   \nplt.rcParams[\"axes.titleweight\"] = 'normal'\nplt.rcParams[\"axes.titlecolor\"] = 'Olive'\nplt.rcParams['axes.edgecolor'] = 'pink'\nplt.rcParams[\"axes.linewidth\"] = 2\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams['axes.titlelocation'] = 'center' \nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams[\"axes.labelpad\"] = 2\nplt.rcParams['axes.labelweight'] = 1\nplt.rcParams[\"axes.labelcolor\"] = 'Olive'\nplt.rcParams[\"axes.axisbelow\"] = False \nplt.rcParams['axes.xmargin'] = .2\nplt.rcParams[\"axes.ymargin\"] = .2\n\n\nplt.rcParams[\"xtick.bottom\"] = True \nplt.rcParams['xtick.color'] = '#A52A2A'\nplt.rcParams[\"ytick.left\"] = True  \nplt.rcParams['ytick.color'] = '#A52A2A'\n\nplt.rcParams['axes.grid'] = True \nplt.rcParams['grid.color'] = 'green'\nplt.rcParams['grid.linestyle'] = '--' \nplt.rcParams['grid.linewidth'] = .5\nplt.rcParams['grid.alpha'] = .3       \n\nplt.rcParams['legend.loc'] = 'best' \nplt.rcParams['legend.facecolor'] =  'NavajoWhite'  \nplt.rcParams['legend.edgecolor'] = 'pink'\nplt.rcParams['legend.shadow'] = True\nplt.rcParams['legend.fontsize'] = 20\n\n\n\nplt.rcParams['font.size'] = 14\n\nplt.rcParams['figure.dpi'] = 200\nplt.rcParams['figure.edgecolor'] = 'Blue'\n\n\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\npd.set_option(\"display.precision\", 2)\n\n\nheart1 = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/classification/heart.csv')\nheart1.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\nNormal\n172\nN\n0.0\nUp\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\nNormal\n156\nN\n1.0\nFlat\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\nST\n98\nN\n0.0\nUp\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\nNormal\n108\nY\n1.5\nFlat\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\nNormal\n122\nN\n0.0\nUp\n0\n\n\n\n\n\n\n\nAttributes:\n\nAge: Age of the patient [years]\nSex: Sex of the patient [M: Male, F: Female]\nChestPainType: Chest Pain Type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\nRestingBP: Resting blood pressure [mm Hg]\nCholesterol: Serum cholesterol [mm/dl]\nFastingBS: Fasting blood sugar [1: if FastingBS &gt; 120 mg/dl, 0: otherwise]\nRestingECG: Resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes’ criteria]\nMaxHR: Maximum heart rate achieved [Numeric value between 60 and 202]\nExerciseAngina: Exercise-induced angina [Y: Yes, N: No]\nOldpeak: Oldpeak = ST [Numeric value measured in depression]\nST_Slope: The slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\nHeartDisease: Output class [1: heart disease, 0: Normal]\n\n\nheart1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 918 entries, 0 to 917\nData columns (total 12 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Age             918 non-null    int64  \n 1   Sex             918 non-null    object \n 2   ChestPainType   918 non-null    object \n 3   RestingBP       918 non-null    int64  \n 4   Cholesterol     918 non-null    int64  \n 5   FastingBS       918 non-null    int64  \n 6   RestingECG      918 non-null    object \n 7   MaxHR           918 non-null    int64  \n 8   ExerciseAngina  918 non-null    object \n 9   Oldpeak         918 non-null    float64\n 10  ST_Slope        918 non-null    object \n 11  HeartDisease    918 non-null    int64  \ndtypes: float64(1), int64(6), object(5)\nmemory usage: 86.2+ KB\n\n\n\nheart1.describe()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nMaxHR\nOldpeak\nHeartDisease\n\n\n\n\ncount\n918.00\n918.00\n918.00\n918.00\n918.00\n918.00\n918.00\n\n\nmean\n53.51\n132.40\n198.80\n0.23\n136.81\n0.89\n0.55\n\n\nstd\n9.43\n18.51\n109.38\n0.42\n25.46\n1.07\n0.50\n\n\nmin\n28.00\n0.00\n0.00\n0.00\n60.00\n-2.60\n0.00\n\n\n25%\n47.00\n120.00\n173.25\n0.00\n120.00\n0.00\n0.00\n\n\n50%\n54.00\n130.00\n223.00\n0.00\n138.00\n0.60\n1.00\n\n\n75%\n60.00\n140.00\n267.00\n0.00\n156.00\n1.50\n1.00\n\n\nmax\n77.00\n200.00\n603.00\n1.00\n202.00\n6.20\n1.00\n\n\n\n\n\n\n\n\nheart1.isnull().mean()*100\n\nAge               0.0\nSex               0.0\nChestPainType     0.0\nRestingBP         0.0\nCholesterol       0.0\nFastingBS         0.0\nRestingECG        0.0\nMaxHR             0.0\nExerciseAngina    0.0\nOldpeak           0.0\nST_Slope          0.0\nHeartDisease      0.0\ndtype: float64\n\n\n\nWomen = heart1.loc[heart1['Sex'] == 'F'][\"HeartDisease\"]\nrate_women = (Women.sum()/len(Women)).round(2)*100\nprint(\"Percentage of Women with probability of HeartDisease:\", rate_women,\"%\")\n\nMen = heart1.loc[heart1['Sex'] == 'M'][\"HeartDisease\"]\nrate_men = (Men.sum()/len(Men)).round(2)*100\nprint(\"Percentage of Men with probability of HeartDisease  :\", rate_men,\"%\")\n\nPercentage of Women with probability of HeartDisease: 26.0 %\nPercentage of Men with probability of HeartDisease  : 63.0 %\n\n\n\nprint(f'We have {heart1.shape[0]} instances with the {heart1.shape[1]-1} features and 1 output variable')\n\nWe have 918 instances with the 11 features and 1 output variable\n\n\n\n## Combining Data\nheart1.agg(\n    {\n       \"Age\": [\"min\", \"max\", \"median\",\"mean\", \"skew\", 'std'],\n        \"RestingBP\": [\"min\", \"max\", \"median\", \"mean\",\"skew\",'std'],\n        \"Cholesterol\": [\"min\", \"max\", \"median\", \"mean\",\"skew\",'std'],\n        \"Oldpeak\": [\"min\", \"max\", \"median\", \"mean\",\"skew\",'std'],\n        \"MaxHR\": [\"min\", \"max\", \"median\", \"mean\",\"skew\",'std']\n    }\n)\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nOldpeak\nMaxHR\n\n\n\n\nmin\n28.00\n0.00\n0.00\n-2.60\n60.00\n\n\nmax\n77.00\n200.00\n603.00\n6.20\n202.00\n\n\nmedian\n54.00\n130.00\n223.00\n0.60\n138.00\n\n\nmean\n53.51\n132.40\n198.80\n0.89\n136.81\n\n\nskew\n-0.20\n0.18\n-0.61\n1.02\n-0.14\n\n\nstd\n9.43\n18.51\n109.38\n1.07\n25.46\n\n\n\n\n\n\n\n\nsns.kdeplot( data=heart1, x=\"Cholesterol\", hue=\"ChestPainType\", fill=True, common_norm=False, palette=\"tab10\", alpha=.5, linewidth=0);\n\n\n\n\n\nsns.displot(data=heart1, x=\"Cholesterol\", hue=\"ChestPainType\", col=\"Sex\", kind=\"kde\");\n\n\n\n\n\nheart1.nunique().plot(kind='bar')\nplt.title('No of unique values in the dataset')\nplt.show()\n\n\n\n\nOutliers in Data\nOutliers represent atypical data points that have the potential to disrupt statistical analyses and challenge their underlying assumptions. Dealing with outliers is a common task for analysts, and deciding how to handle them can be a complex process. While the instinct may be to eliminate outliers to mitigate their impact, this approach is appropriate only in certain circumstances and should not be the default choice\n\nplt.figure(figsize=(14,20))\n\nplt.subplot(5,2,1)\nsns.distplot(heart1['Age'],color='DeepPink')\nplt.subplot(5,2,2)\nsns.boxplot(heart1['Age'],color='DeepPink')\n\nplt.subplot(5,2,3)\nsns.distplot(heart1['RestingBP'],color='DarkSlateGray')\nplt.subplot(5,2,4)\nsns.boxplot(heart1['RestingBP'],color='DarkSlateGray')\n\nplt.subplot(5,2,5)\nsns.distplot(heart1['Cholesterol'],color='Green')\nplt.subplot(5,2,6)\nsns.boxplot(heart1['Cholesterol'],color='Green')\n\nplt.subplot(5,2,7)\nsns.distplot(heart1['MaxHR'],color='Red')\nplt.subplot(5,2,8)\nsns.boxplot(heart1['MaxHR'],color='Red')\n\nplt.subplot(5,2,9)\nsns.distplot(heart1['Oldpeak'],color='Brown')\nplt.subplot(5,2,10)\nsns.boxplot(heart1['Oldpeak'],color='Brown')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n#assigning values to features as X and target as y\nX=heart1.drop([\"HeartDisease\"],axis=1)\ny=heart1[\"HeartDisease\"]\n\n#Set up a standard scaler for the features\ncol_names = list(X.columns)\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in X.columns:\n    if not pd.api.types.is_numeric_dtype(X[col]):\n        X[col] = le.fit_transform(X[col])\n        \ns_scaler = StandardScaler()\nX_df= s_scaler.fit_transform(X)\nX_df = pd.DataFrame(X_df, columns=col_names)   \nX_df.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nAge\n918.0\n-1.08e-16\n1.0\n-2.71\n-0.69\n0.05\n0.69\n2.49\n\n\nSex\n918.0\n-7.74e-18\n1.0\n-1.94\n0.52\n0.52\n0.52\n0.52\n\n\nChestPainType\n918.0\n1.55e-17\n1.0\n-0.82\n-0.82\n-0.82\n1.28\n2.32\n\n\nRestingBP\n918.0\n1.95e-16\n1.0\n-7.15\n-0.67\n-0.13\n0.41\n3.65\n\n\nCholesterol\n918.0\n0.00e+00\n1.0\n-1.82\n-0.23\n0.22\n0.62\n3.70\n\n\nFastingBS\n918.0\n-3.10e-17\n1.0\n-0.55\n-0.55\n-0.55\n-0.55\n1.81\n\n\nRestingECG\n918.0\n9.29e-17\n1.0\n-1.57\n0.02\n0.02\n0.02\n1.60\n\n\nMaxHR\n918.0\n4.95e-16\n1.0\n-3.02\n-0.66\n0.05\n0.75\n2.56\n\n\nExerciseAngina\n918.0\n-3.87e-18\n1.0\n-0.82\n-0.82\n-0.82\n1.21\n1.21\n\n\nOldpeak\n918.0\n1.24e-16\n1.0\n-3.27\n-0.83\n-0.27\n0.57\n4.98\n\n\nST_Slope\n918.0\n7.74e-17\n1.0\n-2.24\n-0.60\n-0.60\n1.05\n1.05\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_df,y,test_size=0.2,random_state=21)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)\n\n\nKNN Classifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nplt.rcParams['figure.figsize'] = (8,6)\nplt.rcParams['font.size'] = 20\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred_log = knn.predict(X_test)\n\nlog_train = round(knn.score(X_train, y_train) * 100, 2)\nlog_accuracy = round(accuracy_score(y_pred_log, y_test) * 100, 2)\nlog_f1 = round(f1_score(y_pred_log, y_test) * 100, 2)\n\nprint(\"Training Accuracy    :\",log_train,\"%\")\nprint(\"Model Accuracy Score :\",log_accuracy,\"%\")\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(\"Classification_Report: \\n\",classification_report(y_test,y_pred_log))\n\nTraining Accuracy    : 88.15 %\nModel Accuracy Score : 86.96 %\n--------------------------------------------------------\nClassification_Report: \n               precision    recall  f1-score   support\n\n           0       0.87      0.80      0.84        76\n           1       0.87      0.92      0.89       108\n\n    accuracy                           0.87       184\n   macro avg       0.87      0.86      0.86       184\nweighted avg       0.87      0.87      0.87       184\n\n\n\n\n\n\nConfusion Matrix (src: https://www.datacamp.com/tutorial/precision-recall-curve-tutorial)\n\n\n\nconfusion_matrix(y_test, y_pred_log);\n\n\nknn_probs = knn.predict_proba(X_test)\nknn_probs = knn_probs[:, 1]\nfrom sklearn.metrics import roc_curve, roc_auc_score\nrf_auc = roc_auc_score(y_test, knn_probs)\nrf_fpr, rf_tpr, _ = roc_curve(y_test, knn_probs)\n\n\nplt.figure(figsize=(12, 8))\nplt.plot(rf_fpr, rf_tpr, linestyle='--', label='AUC = %0.3f' % rf_auc)\n\n\nplt.title('ROC Plot')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()  \nplt.show()\n\n\n\n\n\nprecision, recall, thresholds = precision_recall_curve(y_test, knn_probs)\nplt.fill_between(recall, precision)\nplt.ylabel(\"Precision\")\nplt.xlabel(\"Recall\")\nplt.title(\"Precision-Recall curve\");"
  },
  {
    "objectID": "posts/about.html",
    "href": "posts/about.html",
    "title": "About",
    "section": "",
    "text": "I am Tahmina Sultana Priya, a PhD student in the Computer Science department at Virginia Tech, where I am passionately delving into the fascinating realm of Artificial Intelligence in the context of digital health. This field holds immense promise for revolutionizing the healthcare sector, and my research is dedicated to contributing to this exciting area. I earned my undergraduate degree in Computer Science and Engineering from Khulna University of Engineering & Technology in Bangladesh in 2020, securing the 7th position among 120 students. Following my graduation, I took on a role as a lecturer at Daffodil International University, a private educational institution in Bangladesh. During my tenure as a faculty member, I had the privilege of teaching various courses such as Data Mining, Machine Learning, Artificial Intelligence, Introduction to Bioinformatics, and Big Data, which allowed me to share my knowledge and passion with the next generation of computer scientists. In addition to my academic responsibilities, I also worked as a part-time Senior DevOps Engineer for Vertical Innovations Limited, gaining practical experience in the field of DevOps. My journey in both academia and industry has provided me with a well-rounded foundation to pursue my current research interests in the fascinating intersection of AI and digital health. The machine learning course holds significant importance in my academic journey as it equips me with the fundamental knowledge and concepts that are directly applicable to my ongoing research in the field of Artificial Intelligence and digital health. Understanding the intricacies of machine learning is not only a valuable asset for my academic pursuits but is also highly relevant to the core of my research work. It provides me with the necessary tools and insights to explore innovative ways in which machine learning can be leveraged to advance the frontiers of healthcare technology and contribute to the betterment of the healthcare sector.\n---\ntitle: \"Beatriz Milz\"\ncomments: false\nimage: bea.jpg\nabout:\n  template: trestles\n  links:\n    - icon: envelope\n      text: Email\n      href: mailto:milz.bea@gmail.com\n    - icon: file-pdf\n      text: Resume\n      href: https://beatrizmilz.github.io/resume/index.pdf  \n    - icon: github\n      text: GitHub\n      href: https://github.com/beatrizmilz\n    - icon: twitter\n      text: Twitter\n      href: https://twitter.com/BeaMilz      \n---\n&lt;center&gt;\n::: {.btn-group role=\"group\" aria-label=\"About\"}\n&lt;a href=\"about.html\"&gt; &lt;button type=\"button\" class=\"btn btn-primary me-md-2 btn-sm\"&gt;English&lt;/button&gt;&lt;/a&gt; &lt;a href=\"about-pt.html\"&gt; &lt;button type=\"button\" class=\"btn btn-primary me-md-2 btn-sm\"&gt;Português&lt;/button&gt;&lt;/a&gt;   &lt;a href=\"about-es.html\"&gt;&lt;button type=\"button\" class=\"btn btn-primary me-md-2 btn-sm\"&gt;Español&lt;/button&gt;&lt;/a&gt;\n:::\n&lt;/center&gt;\n::: {#hero-heading}\nHi!\nMy name is Beatriz, and I am a R-Lady from Brazil 🇧🇷 💜.\nI'm a [tidyverse instructor certified by RStudio](https://education.rstudio.com/trainers/people/milz+beatriz/), and I teach R at [Curso-R](https://curso-r.com/).\nI have been writing about R in my blog since 2019.\nI'm mostly active in the Brazilian R community, co-organizing communities and events, such as: [R-Ladies São Paulo](https://twitter.com/RLadiesSaoPaulo), [Latin-R](https://twitter.com/LatinR_Conf), and [satRday São Paulo](https://twitter.com/satRdaySP).\nI'm a PhD Candidate at the [University of São Paulo](http://www.iee.usp.br/), in Environmental Sciences.\nMy research is about **transparency of information** in the **management of water resources** that are used to supply the Macrometropolis of Sao Paulo.\n:::"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Tahmina, and I am from Bangladesh.\nI am a PhD student in the Computer Science department at Virginia Tech, where I am passionately delving into the fascinating realm of Artificial Intelligence in the context of digital health. This field holds immense promise for revolutionizing the healthcare sector, and my research is dedicated to contributing to this exciting area.\nThe machine learning course holds significant importance in my academic journey as it equips me with the fundamental knowledge and concepts that are directly applicable to my ongoing research in the field of Artificial Intelligence and digital health. Understanding the intricacies of machine learning is not only a valuable asset for my academic pursuits but is also highly relevant to the core of my research work. It provides me with the necessary tools and insights to explore innovative ways in which machine learning can be leveraged to advance the frontiers of healthcare technology and contribute to the betterment of the healthcare sector."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tahmina Sultana Priya",
    "section": "",
    "text": "python\n\n\ncode\n\n\nanalysis\n\n\n\n\nClassification in Machine Learning involves the process of assigning predefined categories or labels to data points based on their features, enabling algorithms to learn and predict the class of new, unseen instances.\n\n\n\n\n\n\nNov 24, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nProbability in Machine Learning enables the quantification of uncertainty, allowing algorithms to make informed decisions by assessing the likelihood of various outcomes or events occurring based on available data.\n\n\n\n\n\n\nNov 22, 2023\n\n\nTahmina Sultana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nAnomaly detection in Machine Learning focuses on identifying rare or unusual instances in data that significantly differ from the majority of normal observations, aiding in the detection of outliers or irregular patterns.\n\n\n\n\n\n\nNov 20, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nLinear Regression is a supervised learning algorithm used for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n\n\n\n\n\n\nNov 5, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nNonlinear Regression involves modeling the relationship between dependent and independent variables using a nonlinear function, allowing for more complex and flexible patterns beyond linear relationships.\n\n\n\n\n\n\nOct 29, 2023\n\n\nTahmina Sultana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nClustering in Machine Learning involves grouping similar data points together based on certain features or characteristics, aiming to discover inherent patterns or structures within the data.\n\n\n\n\n\n\nOct 28, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Tahmina Sultana Priya",
    "section": "",
    "text": "python\n\n\ncode\n\n\nanalysis\n\n\n\n\nClassification in Machine Learning involves the process of assigning predefined categories or labels to data points based on their features, enabling algorithms to learn and predict the class of new, unseen instances.\n\n\n\n\n\n\nNov 24, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nProbability in Machine Learning enables the quantification of uncertainty, allowing algorithms to make informed decisions by assessing the likelihood of various outcomes or events occurring based on available data.\n\n\n\n\n\n\nNov 22, 2023\n\n\nTahmina Sultana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nAnomaly detection in Machine Learning focuses on identifying rare or unusual instances in data that significantly differ from the majority of normal observations, aiding in the detection of outliers or irregular patterns.\n\n\n\n\n\n\nNov 20, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nLinear Regression is a supervised learning algorithm used for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n\n\n\n\n\n\nNov 5, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nNonlinear Regression involves modeling the relationship between dependent and independent variables using a nonlinear function, allowing for more complex and flexible patterns beyond linear relationships.\n\n\n\n\n\n\nOct 29, 2023\n\n\nTahmina Sultana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nClustering in Machine Learning involves grouping similar data points together based on certain features or characteristics, aiming to discover inherent patterns or structures within the data.\n\n\n\n\n\n\nOct 28, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nlinregression/index.html",
    "href": "posts/nlinregression/index.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Nonlinear Regression\n\n\nNonlinear regression is a statistical technique used to model data by expressing it as a mathematical function. Unlike simple linear regression, which uses a straight line to relate two variables (X and Y), nonlinear regression captures more complex, curved relationships between these variables.\nThe main objective of nonlinear regression is to minimize the sum of squared differences between the observed Y values and the predictions made by the nonlinear model. This sum of squares serves as a measure of how well the model fits the data points. To compute it, we calculate the differences between the fitted nonlinear function and each data point’s Y value, square these differences, and then sum them up. A smaller sum of squared differences indicates a better fit of the model to the data.\nNonlinear regression employs various mathematical functions such as logarithmic, trigonometric, exponential, power functions, Lorenz curves, Gaussian functions, and other fitting techniques to capture the underlying relationships in the data.\nKey Notes:\n\nLinear and nonlinear regression are methods for predicting Y values based on an X variable (or multiple X variables).\nNonlinear regression involves using a curved mathematical function of X variables to make predictions for a Y variable.\nNonlinear regression can be employed to model and predict population growth trends over time.\n\nNonlinear regression modeling and linear regression modeling both aim to visually represent a specific response based on a set of variables. Nonlinear models are more intricate to construct compared to linear models because they involve approximations, often through trial-and-error iterations. Mathematicians employ established techniques like the Gauss-Newton method and the Levenberg-Marquardt method in this process.\nSometimes, what may initially appear as a nonlinear regression model can, in fact, be linear. To determine the underlying relationships in your data, you can use curve estimation procedures to choose the appropriate regression model, be it linear or nonlinear. Linear regression models, typically represented as straight lines, can also exhibit curves depending on the specific linear equation used. Additionally, it’s possible to apply algebraic transformations to make a nonlinear equation resemble a linear one, termed as an “intrinsically linear” nonlinear equation.\n\n\nI.Introduction\nIf the data shows a curvy trend, then linear regression will not produce very accurate results when compared to a non-linear regression because, as the name implies, linear regression presumes that the data is linear.\nImporting required libraries\n\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization\n\n\n\n\n\nx = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 3*(x) + 2\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\n\n\n\nx = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 1*(x**3) + 2*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\n\n\n\nx = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\n\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\n\n\n\nX = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\n\nY= np.exp(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\n\n\n\nX = np.arange(1.0, 10.0, 0.1)\n\nY = np.log(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\n\n\n\nX = np.arange(-5.0, 5.0, 0.1)\n\n\nY = 1-4/(1+np.power(3, X-2))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\n\n\n\npath = '../input/'\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/nlinregression/gdp.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n\n\nNumber of rows and columns in the data set:  (55, 2)\n\n\n\n\n\n\n\n\n\n\nYear\nValue\n\n\n\n\n0\n1960\n5.918412e+10\n\n\n1\n1961\n4.955705e+10\n\n\n2\n1962\n4.668518e+10\n\n\n3\n1963\n5.009730e+10\n\n\n4\n1964\n5.906225e+10\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,5))\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\n\n\n\nChoosing a model\nFrom an initial look at the plot, we determine that the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\n\nX = np.arange(-5,5.0, 0.1)\nY = 1.0 / (1.0 + np.exp(-X))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\n\nNow, let’s build our regression model and initialize its parameters.\n\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n   \n   \nbeta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\n\n\n\n\n\n# Lets normalize our data\nxdata =x_data/max(x_data)\nydata =y_data/max(y_data)\n\n\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 690.451709, beta_2 = 0.997207\n\n\n\nx = np.linspace(1960, 2015, 55)\nx = x/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\n\n# split data into train/test\nmsk = np.random.rand(len(df)) &lt; 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )\n\nMean absolute error: 0.02\nResidual sum of squares (MSE): 0.00\nR2-score: 0.94"
  },
  {
    "objectID": "posts/nlinregression/index.html#example-of-nonlinear-regression",
    "href": "posts/nlinregression/index.html#example-of-nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "I.Introduction\nIf the data shows a curvy trend, then linear regression will not produce very accurate results when compared to a non-linear regression because, as the name implies, linear regression presumes that the data is linear.\nImporting required libraries\n\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization"
  },
  {
    "objectID": "posts/nlinregression/index.html#linear",
    "href": "posts/nlinregression/index.html#linear",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "x = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 3*(x) + 2\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#polynomial",
    "href": "posts/nlinregression/index.html#polynomial",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "x = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 1*(x**3) + 2*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#quadratic",
    "href": "posts/nlinregression/index.html#quadratic",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "x = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\n\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#exponential",
    "href": "posts/nlinregression/index.html#exponential",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "X = np.arange(-6.0, 6.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\n\nY= np.exp(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#logarithmic",
    "href": "posts/nlinregression/index.html#logarithmic",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "X = np.arange(1.0, 10.0, 0.1)\n\nY = np.log(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#sigmoidallogistic",
    "href": "posts/nlinregression/index.html#sigmoidallogistic",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "X = np.arange(-5.0, 5.0, 0.1)\n\n\nY = 1-4/(1+np.power(3, X-2))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#non-linear-regression-example-with-dataset",
    "href": "posts/nlinregression/index.html#non-linear-regression-example-with-dataset",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "path = '../input/'\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/nlinregression/gdp.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n\n\nNumber of rows and columns in the data set:  (55, 2)\n\n\n\n\n\n\n\n\n\n\nYear\nValue\n\n\n\n\n0\n1960\n5.918412e+10\n\n\n1\n1961\n4.955705e+10\n\n\n2\n1962\n4.668518e+10\n\n\n3\n1963\n5.009730e+10\n\n\n4\n1964\n5.906225e+10\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,5))\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#section",
    "href": "posts/nlinregression/index.html#section",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Choosing a model\nFrom an initial look at the plot, we determine that the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\n\nX = np.arange(-5,5.0, 0.1)\nY = 1.0 / (1.0 + np.exp(-X))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\n\nNow, let’s build our regression model and initialize its parameters.\n\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n   \n   \nbeta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\n\n\n\n\n\n# Lets normalize our data\nxdata =x_data/max(x_data)\nydata =y_data/max(y_data)\n\n\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 690.451709, beta_2 = 0.997207\n\n\n\nx = np.linspace(1960, 2015, 55)\nx = x/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\n\n# split data into train/test\nmsk = np.random.rand(len(df)) &lt; 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )\n\nMean absolute error: 0.02\nResidual sum of squares (MSE): 0.00\nR2-score: 0.94"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "MLBlog",
    "section": "",
    "text": "No matching items"
  }
]