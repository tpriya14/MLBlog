[
  {
    "objectID": "posts/linregression/index.html",
    "href": "posts/linregression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Contents:\n\nWhat Is Linear Regression?\nExample of Linear Regression with US Health Insurance dataset.\nData Preprocessing\nData Visualization\nModel Implementation\nEvaluation"
  },
  {
    "objectID": "posts/linregression/index.html#import-library-and-dataset",
    "href": "posts/linregression/index.html#import-library-and-dataset",
    "title": "Linear Regression",
    "section": "Import Library and Dataset",
    "text": "Import Library and Dataset\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n# Import library\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Import dataset\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/linregression/insurance.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\ndf.head()\n\n\nNumber of rows and columns in the data set:  (1338, 7)\n\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n\n\n\n\n\n\n\"\"\" for our visualization purpose will fit line using seaborn library only for bmi as independent variable \nand charges as dependent variable\"\"\"\n\nsns.lmplot(x='bmi',y='charges',data=df,aspect=1.5,  height=4)\nplt.xlabel('Boby Mass Index$(kg/m^2)$: as Independent variable')\nplt.ylabel('Insurance Charges: as Dependent variable')\nplt.title('Charge Vs BMI');\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\ncharges\n\n\n\n\ncount\n1338.000000\n1338.000000\n1338.000000\n1338.000000\n\n\nmean\n39.207025\n30.663397\n1.094918\n13270.422265\n\n\nstd\n14.049960\n6.098187\n1.205493\n12110.011237\n\n\nmin\n18.000000\n15.960000\n0.000000\n1121.873900\n\n\n25%\n27.000000\n26.296250\n0.000000\n4740.287150\n\n\n50%\n39.000000\n30.400000\n1.000000\n9382.033000\n\n\n75%\n51.000000\n34.693750\n2.000000\n16639.912515\n\n\nmax\n64.000000\n53.130000\n5.000000\n63770.428010\n\n\n\n\n\n\n\n\nCheck for missing value\n\n# Check for missing values in the DataFrame\nmissing_values = df.isnull().sum()\nprint(\"Missing values count per column:\")\nprint(missing_values)\n\nMissing values count per column:\nage         0\nsex         0\nbmi         0\nchildren    0\nsmoker      0\nregion      0\ncharges     0\ndtype: int64\n\n\n\nf= plt.figure(figsize=(9,4))\n\nax=f.add_subplot(121)\nsns.histplot(df['charges'],bins=50,color='r',ax=ax)\nax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.histplot(np.log10(df['charges']),bins=40,color='b',ax=ax)\nax.set_title('Distribution of insurance charges in $log$ sacle')\nax.set_xscale('log');\n\n\n\n\n\nf = plt.figure(figsize=(9,6))\nax = f.add_subplot(121)\nsns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\nax.set_title('Violin plot of Charges vs sex')\n\nax = f.add_subplot(122)\nsns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\nax.set_title('Violin plot of Charges vs smoker');\n\n\n\n\nFrom left plot the insurance charge for male and female is approximatley in same range,it is average around 5000 bucks. In right plot the insurance charge for smokers is much wide range compare to non smokers, the average charges for non smoker is approximately 5000 bucks. For smoker the minimum insurance charge is itself 5000 bucks.\n\nplt.figure(figsize=(9,6))\nsns.boxplot(x='children', y='charges',hue='sex',data=df,palette='rainbow')\nplt.title('Box plot of charges vs children');\n\n\n\n\n\nplt.figure(figsize=(9,6))\nsns.violinplot(x='region', y='charges',hue='sex',data=df,palette='rainbow',split=True)\nplt.title('Violin plot of charges vs children');\n\n\n\n\n\nf = plt.figure(figsize=(9,6))\nax = f.add_subplot(121)\nsns.scatterplot(x='age',y='charges',data=df,palette='magma',hue='smoker',ax=ax)\nax.set_title('Scatter plot of Charges vs age')\n\nax = f.add_subplot(122)\nsns.scatterplot(x='bmi',y='charges',data=df,palette='viridis',hue='smoker')\nax.set_title('Scatter plot of Charges vs bmi')\nplt.savefig('sc.png');"
  },
  {
    "objectID": "posts/linregression/index.html#data-preprocessing",
    "href": "posts/linregression/index.html#data-preprocessing",
    "title": "Linear Regression",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nData preprocessing in machine learning involves encoding categorical data into numerical form, as machine learning algorithms typically require numerical input. There are several techniques for this:\n\nLabel Encoding: This method involves converting categorical labels into numerical values to enable algorithms to work with them.\nOne-Hot Encoding: One-hot encoding represents categorical variables as binary vectors, making the data more expressive. First, the categorical values are mapped to integer values (label encoding), and then each integer is converted into a binary vector with all zeros except for the index of the integer, which is marked with a 1.\nDummy Variable Trap: This situation occurs when independent variables are multicollinear, meaning that two or more variables are highly correlated, making it possible to predict one variable from the others.\n\nTo simplify this process, the pandas library offers a convenient function called get_dummies. This function allows us to perform all three steps in a single line of code. We can use it to create dummy variables for features like ‘sex,’ ‘children,’ ‘smoker,’ and ‘region.’ By setting the drop_first=True parameter, we can automatically eliminate the dummy variable trap by dropping one variable and retaining the original variable. This makes data preprocessing more straightforward and efficient.\n\n# Dummy variable\ncategorical_columns = ['sex','children', 'smoker', 'region']\ndf_encode = pd.get_dummies(data = df, prefix = 'OHE', prefix_sep='_',\n               columns = categorical_columns,\n               drop_first =True,\n              dtype='int8')\n\n\n# Lets verify the dummay variable process\nprint('Columns in original data frame:\\n',df.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df.shape)\nprint('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df_encode.shape)\n\nColumns in original data frame:\n ['age' 'sex' 'bmi' 'children' 'smoker' 'region' 'charges']\n\nNumber of rows and columns in the dataset: (1338, 7)\n\nColumns in data frame after encoding dummy variable:\n ['age' 'bmi' 'charges' 'OHE_male' 'OHE_1' 'OHE_2' 'OHE_3' 'OHE_4' 'OHE_5'\n 'OHE_yes' 'OHE_northwest' 'OHE_southeast' 'OHE_southwest']\n\nNumber of rows and columns in the dataset: (1338, 13)\n\n\n\nfrom scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\n\n#df['charges'] = y_bc  \n# it did not perform better for this model, so log transform is used\nci,lam\n## Log transform\ndf_encode['charges'] = np.log(df_encode['charges'])\n\nThe original categorical variable are remove and also one of the one hot encode varible column for perticular categorical variable is droped from the column. So we completed all three encoding step by using get dummies function."
  },
  {
    "objectID": "posts/linregression/index.html#train-test-split",
    "href": "posts/linregression/index.html#train-test-split",
    "title": "Linear Regression",
    "section": "Train Test split",
    "text": "Train Test split\n\nfrom sklearn.model_selection import train_test_split\nX = df_encode.drop('charges',axis=1) # Independet variable\ny = df_encode['charges'] # dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)\n\n\n# Step 1: add x0 =1 to dataset\nX_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\nX_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n\n# Step2: build model\ntheta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) \n\n\n# The parameters for linear regression model\nparameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\ncolumns = ['intersect:x_0=1'] + list(X.columns.values)\nparameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})\n\n\n# Scikit Learn module\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.\n\n#Parameter\nsk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\nparameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\n\nThe parameter obtained from both the model are same.So we successfully build our model using normal equation and verified using sklearn linear regression module. Let’s move ahead, next step is prediction and model evaluation.\n\n# Normal equation\ny_pred_norm =  np.matmul(X_test_0,theta)\n\n#Evaluvation: MSE\nJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n\n# R_square \nsse = np.sum((y_pred_norm - y_test)**2)\nsst = np.sum((y_test - y_test.mean())**2)\nR_square = 1 - (sse/sst)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\nprint('R square obtain for normal equation method is :',R_square)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.18729622322982042\nR square obtain for normal equation method is : 0.7795687545055301\n\n\n\n# sklearn regression module\ny_pred_sk = lin_reg.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk = mean_squared_error(y_pred_sk, y_test)\n\n# R_square\nR_square_sk = lin_reg.score(X_test,y_test)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\nprint('R square obtain for scikit learn library is :',R_square_sk)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.1872962232298189\nR square obtain for scikit learn library is : 0.7795687545055319\n\n\nModel validation is a crucial step in assessing the performance of a linear regression model, and it involves checking various assumptions. The key assumptions for a linear regression model are as follows:\n\nLinear Relationship: Linear regression assumes that the relationship between the dependent and independent variables is linear. You can verify this assumption by creating a scatter plot of actual values against predicted values.\nNormality of Residuals: The residual errors should follow a normal distribution. This can be checked by examining the distribution of the residuals.\nMean of Residuals: The mean of the residual errors should ideally be close to 0.\nMultivariate Normality: Linear regression assumes that all variables are multivariate normally distributed. This assumption can be assessed using a Q-Q plot.\nMulticollinearity: Linear regression assumes minimal multicollinearity, meaning that independent variables are not highly correlated with each other. The variance inflation factor (VIF) can help identify and measure the strength of such correlations. A VIF greater than 1 but less than 5 indicates moderate correlation, while a VIF less than 5 suggests a critical level of multicollinearity.\nHomoscedasticity: The data should exhibit homoscedasticity, which means that the residuals are roughly equal across the regression line. You can assess this by creating a scatter plot of residuals against the fitted values. If the plot shows a funnel-shaped pattern, it indicates heteroscedasticity.\n\nEnsuring these assumptions are met is essential to build a reliable linear regression model.\n\n# Check for Linearity\nf = plt.figure(figsize=(9,5))\nax = f.add_subplot(121)\n#sns.scatterplot(data=df, y_test, y_pred_sk)\nsns.scatterplot(x=y_test,y=y_pred_sk,ax=ax,color='r')\nax.set_title('Check for Linearity:\\n Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\nsns.histplot((y_test - y_pred_sk),ax=ax,color='b')\nax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror');\n\n\n\n\n\n# Check for Multivariate Normality\n# Quantile-Quantile plot \nf,ax = plt.subplots(1,2,figsize=(9,6))\nimport scipy as sp\n_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])\nax[0].set_title('Check for Multivariate Normality: \\nQ-Q Plot')\n\n#Check for Homoscedasticity\nsns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') \nax[1].set_title('Check for Homoscedasticity: \\nResidual Vs Predicted');\n\n\n\n\n\n# Check for Multicollinearity\n#Variance Inflation Factor\nVIF = 1/(1- R_square_sk)\nVIF\n\n4.536561945911138\n\n\nHere are the model assumptions for linear regression, along with their assessment:\n\nThe actual vs. predicted plot doesn’t form a linear pattern, indicating a failure of the linear assumption.\nThe mean of the residuals is close to zero, and the residual error plot is skewed to the right.\nThe Q-Q plot shows that values greater than 1.5 tend to increase, suggesting a departure from multivariate normality.\nThe plot exhibits heteroscedasticity, with errors increasing after a certain point.\nThe variance inflation factor is less than 5, indicating the absence of multicollinearity."
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Machine Learning Algorithms for Classification (src: https://towardsdatascience.com/top-machine-learning-algorithms-for-classification-2197870ff501)\nContents:"
  },
  {
    "objectID": "posts/classification/index.html#introduction-to-classification",
    "href": "posts/classification/index.html#introduction-to-classification",
    "title": "Classification",
    "section": "Introduction to Classification",
    "text": "Introduction to Classification\nIn the dynamic world of machine learning, classification stands out as a pivotal concept, providing the ability to categorize and interpret data for a wide array of applications. Whether you’re looking to filter spam emails, identify diseases from medical data, or recognize handwritten digits, classification algorithms are your go-to tools. In this blog post, we’ll delve into what classification is in machine learning, its significance, and how it’s transforming industries."
  },
  {
    "objectID": "posts/classification/index.html#defining-classification-in-machine-learning",
    "href": "posts/classification/index.html#defining-classification-in-machine-learning",
    "title": "Classification",
    "section": "Defining Classification in Machine Learning",
    "text": "Defining Classification in Machine Learning\nAt its core, classification is the process of recognizing, understanding, and grouping data into predefined categories or subgroups. It’s like having a smart assistant that can look at a new piece of data and tell you which group it belongs to based on its characteristics. This task is accomplished through the analysis of historical data, which serves as a training ground for machine learning models."
  },
  {
    "objectID": "posts/classification/index.html#the-power-of-predefined-categories",
    "href": "posts/classification/index.html#the-power-of-predefined-categories",
    "title": "Classification",
    "section": "The Power of Predefined Categories",
    "text": "The Power of Predefined Categories\nThe magic of classification lies in these predefined categories. Think of them as labels, such as “spam” and “non-spam” for emails, “fraudulent” and “legitimate” for financial transactions, or “cat” and “dog” for image recognition. The ability to organize data into these categories enables decision-making, automation, and insights that would otherwise be impractical or impossible to achieve manually."
  },
  {
    "objectID": "posts/classification/index.html#how-classification-works",
    "href": "posts/classification/index.html#how-classification-works",
    "title": "Classification",
    "section": "How Classification Works",
    "text": "How Classification Works\nTo perform classification, machine learning algorithms need to learn from data first. This “training” phase involves feeding the algorithm a labeled dataset, where each data point is associated with the category it belongs to. The algorithm then learns the patterns, relationships, and features that characterize each category.\nOnce trained, the algorithm can classify new, unseen data by assessing its similarity to the patterns it has learned. It predicts the likelihood of the new data point falling into one of the predefined categories. This process is akin to your email provider recognizing whether an incoming email is spam or not based on past experiences."
  },
  {
    "objectID": "posts/classification/index.html#real-life-applications",
    "href": "posts/classification/index.html#real-life-applications",
    "title": "Classification",
    "section": "Real-Life Applications",
    "text": "Real-Life Applications\nClassification has found its way into countless real-world applications. From medical diagnoses to recommendation systems, here are a few examples:\n\nMedical Diagnoses: Doctors use machine learning models to predict whether a patient has a particular disease based on symptoms, medical history, and test results.\nRecommendation Systems: Companies like Netflix and Amazon employ classification to recommend movies or products to users based on their preferences and behavior.\nSentiment Analysis: Social media platforms analyze posts to classify them as positive, negative, or neutral, providing valuable insights into public opinion.\nImage Recognition: In the field of computer vision, classification helps identify objects, animals, or handwritten text in images.\n\n\nPopular Classification Algorithms:\n\nLogistic Regression: Logistic regression is a widely used classification algorithm that models the probability of an input belonging to a particular category. It’s simple, interpretable, and effective for binary and multiclass classification tasks.\nNaive Bayes: Naive Bayes is a probabilistic classification algorithm based on Bayes’ theorem. It’s particularly suited for text classification tasks and spam email filtering, where it assumes independence between features.\nK-Nearest Neighbors: K-NN is a straightforward yet powerful algorithm that classifies data points based on the majority class among their k-nearest neighbors. It’s versatile and can be applied to various types of data, but the choice of k is crucial for its performance.\nDecision Tree: Decision tree classifiers make decisions by splitting data based on features, creating a tree-like structure of decisions. They are interpretable and can handle both categorical and numerical data, making them useful in many applications.\nSupport Vector Machines: SVMs are effective for both linear and nonlinear classification tasks. They work by finding the optimal hyperplane that maximizes the margin between classes, making them robust against overfitting and suitable for high-dimensional data."
  },
  {
    "objectID": "posts/classification/index.html#example-heart-disease-prediction",
    "href": "posts/classification/index.html#example-heart-disease-prediction",
    "title": "Classification",
    "section": "Example: Heart Disease Prediction",
    "text": "Example: Heart Disease Prediction\n\nimport pandas as pd \nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import roc_curve, precision_recall_curve\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, f1_score, accuracy_score\nfrom sklearn.metrics import make_scorer, precision_score, precision_recall_curve\nfrom sklearn.metrics import recall_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nsns.set_theme(context='notebook',\n              style='white',\n              palette='deep',\n              font_scale=1.5,\n              color_codes=True,\n              rc=None)\n\nimport matplotlib\n\nplt.rcParams['figure.figsize'] = (14,8) \nplt.rcParams['figure.facecolor'] = '#F0F8FF'\nplt.rcParams['figure.titlesize'] = 'medium'\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['figure.edgecolor'] = 'green'\nplt.rcParams['figure.frameon'] = True\n\nplt.rcParams[\"figure.autolayout\"] = True\n\nplt.rcParams['axes.facecolor'] = '#F5F5DC'\nplt.rcParams['axes.titlesize'] = 25   \nplt.rcParams[\"axes.titleweight\"] = 'normal'\nplt.rcParams[\"axes.titlecolor\"] = 'Olive'\nplt.rcParams['axes.edgecolor'] = 'pink'\nplt.rcParams[\"axes.linewidth\"] = 2\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams['axes.titlelocation'] = 'center' \nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams[\"axes.labelpad\"] = 2\nplt.rcParams['axes.labelweight'] = 1\nplt.rcParams[\"axes.labelcolor\"] = 'Olive'\nplt.rcParams[\"axes.axisbelow\"] = False \nplt.rcParams['axes.xmargin'] = .2\nplt.rcParams[\"axes.ymargin\"] = .2\n\n\nplt.rcParams[\"xtick.bottom\"] = True \nplt.rcParams['xtick.color'] = '#A52A2A'\nplt.rcParams[\"ytick.left\"] = True  \nplt.rcParams['ytick.color'] = '#A52A2A'\n\nplt.rcParams['axes.grid'] = True \nplt.rcParams['grid.color'] = 'green'\nplt.rcParams['grid.linestyle'] = '--' \nplt.rcParams['grid.linewidth'] = .5\nplt.rcParams['grid.alpha'] = .3       \n\nplt.rcParams['legend.loc'] = 'best' \nplt.rcParams['legend.facecolor'] =  'NavajoWhite'  \nplt.rcParams['legend.edgecolor'] = 'pink'\nplt.rcParams['legend.shadow'] = True\nplt.rcParams['legend.fontsize'] = 20\n\nplt.rcParams['font.size'] = 14\n\nplt.rcParams['figure.dpi'] = 200\nplt.rcParams['figure.edgecolor'] = 'Blue'\n\n\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\npd.set_option(\"display.precision\", 2)\n\n\nheart1 = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/classification/heart.csv')\nheart1.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\nNormal\n172\nN\n0.0\nUp\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\nNormal\n156\nN\n1.0\nFlat\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\nST\n98\nN\n0.0\nUp\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\nNormal\n108\nY\n1.5\nFlat\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\nNormal\n122\nN\n0.0\nUp\n0\n\n\n\n\n\n\n\nAttributes:\n\nAge: Age of the patient [years]\nSex: Sex of the patient [M: Male, F: Female]\nChestPainType: Chest Pain Type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\nRestingBP: Resting blood pressure [mm Hg]\nCholesterol: Serum cholesterol [mm/dl]\nFastingBS: Fasting blood sugar [1: if FastingBS &gt; 120 mg/dl, 0: otherwise]\nRestingECG: Resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes’ criteria]\nMaxHR: Maximum heart rate achieved [Numeric value between 60 and 202]\nExerciseAngina: Exercise-induced angina [Y: Yes, N: No]\nOldpeak: Oldpeak = ST [Numeric value measured in depression]\nST_Slope: The slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\nHeartDisease: Output class [1: heart disease, 0: Normal]\n\n\nheart1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 918 entries, 0 to 917\nData columns (total 12 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Age             918 non-null    int64  \n 1   Sex             918 non-null    object \n 2   ChestPainType   918 non-null    object \n 3   RestingBP       918 non-null    int64  \n 4   Cholesterol     918 non-null    int64  \n 5   FastingBS       918 non-null    int64  \n 6   RestingECG      918 non-null    object \n 7   MaxHR           918 non-null    int64  \n 8   ExerciseAngina  918 non-null    object \n 9   Oldpeak         918 non-null    float64\n 10  ST_Slope        918 non-null    object \n 11  HeartDisease    918 non-null    int64  \ndtypes: float64(1), int64(6), object(5)\nmemory usage: 86.2+ KB\n\n\n\nheart1.describe()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nMaxHR\nOldpeak\nHeartDisease\n\n\n\n\ncount\n918.00\n918.00\n918.00\n918.00\n918.00\n918.00\n918.00\n\n\nmean\n53.51\n132.40\n198.80\n0.23\n136.81\n0.89\n0.55\n\n\nstd\n9.43\n18.51\n109.38\n0.42\n25.46\n1.07\n0.50\n\n\nmin\n28.00\n0.00\n0.00\n0.00\n60.00\n-2.60\n0.00\n\n\n25%\n47.00\n120.00\n173.25\n0.00\n120.00\n0.00\n0.00\n\n\n50%\n54.00\n130.00\n223.00\n0.00\n138.00\n0.60\n1.00\n\n\n75%\n60.00\n140.00\n267.00\n0.00\n156.00\n1.50\n1.00\n\n\nmax\n77.00\n200.00\n603.00\n1.00\n202.00\n6.20\n1.00\n\n\n\n\n\n\n\n\nheart1.isnull().mean()*100\n\nAge               0.0\nSex               0.0\nChestPainType     0.0\nRestingBP         0.0\nCholesterol       0.0\nFastingBS         0.0\nRestingECG        0.0\nMaxHR             0.0\nExerciseAngina    0.0\nOldpeak           0.0\nST_Slope          0.0\nHeartDisease      0.0\ndtype: float64\n\n\n\nWomen = heart1.loc[heart1['Sex'] == 'F'][\"HeartDisease\"]\nrate_women = (Women.sum()/len(Women)).round(2)*100\nprint(\"Percentage of Women with probability of HeartDisease:\", rate_women,\"%\")\n\nMen = heart1.loc[heart1['Sex'] == 'M'][\"HeartDisease\"]\nrate_men = (Men.sum()/len(Men)).round(2)*100\nprint(\"Percentage of Men with probability of HeartDisease  :\", rate_men,\"%\")\n\nPercentage of Women with probability of HeartDisease: 26.0 %\nPercentage of Men with probability of HeartDisease  : 63.0 %\n\n\n\nprint(f'We have {heart1.shape[0]} instances with the {heart1.shape[1]-1} features and 1 output variable')\n\nWe have 918 instances with the 11 features and 1 output variable\n\n\n\n## Combining Data\nheart1.agg(\n    {\n       \"Age\": [\"min\", \"max\", \"median\",\"mean\", \"skew\", 'std'],\n        \"RestingBP\": [\"min\", \"max\", \"median\", \"mean\",\"skew\",'std'],\n        \"Cholesterol\": [\"min\", \"max\", \"median\", \"mean\",\"skew\",'std'],\n        \"Oldpeak\": [\"min\", \"max\", \"median\", \"mean\",\"skew\",'std'],\n        \"MaxHR\": [\"min\", \"max\", \"median\", \"mean\",\"skew\",'std']\n    }\n)\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nOldpeak\nMaxHR\n\n\n\n\nmin\n28.00\n0.00\n0.00\n-2.60\n60.00\n\n\nmax\n77.00\n200.00\n603.00\n6.20\n202.00\n\n\nmedian\n54.00\n130.00\n223.00\n0.60\n138.00\n\n\nmean\n53.51\n132.40\n198.80\n0.89\n136.81\n\n\nskew\n-0.20\n0.18\n-0.61\n1.02\n-0.14\n\n\nstd\n9.43\n18.51\n109.38\n1.07\n25.46\n\n\n\n\n\n\n\n\nsns.kdeplot( data=heart1, x=\"Cholesterol\", hue=\"ChestPainType\", fill=True, common_norm=False, palette=\"tab10\", alpha=.5, linewidth=0);\n\n\n\n\n\nsns.displot(data=heart1, x=\"Cholesterol\", hue=\"ChestPainType\", col=\"Sex\", kind=\"kde\");\n\n\n\n\n\nheart1.nunique().plot(kind='bar')\nplt.title('No of unique values in the dataset')\nplt.show()\n\n\n\n\nOutliers in Data\nOutliers represent atypical data points that have the potential to disrupt statistical analyses and challenge their underlying assumptions. Dealing with outliers is a common task for analysts, and deciding how to handle them can be a complex process. While the instinct may be to eliminate outliers to mitigate their impact, this approach is appropriate only in certain circumstances and should not be the default choice\n\nplt.figure(figsize=(14,20))\nplt.subplot(5,2,1)\nsns.distplot(heart1['Age'],color='DeepPink')\nplt.subplot(5,2,2)\nsns.boxplot(heart1['Age'],color='DeepPink')\nplt.subplot(5,2,3)\nsns.distplot(heart1['RestingBP'],color='DarkSlateGray')\nplt.subplot(5,2,4)\nsns.boxplot(heart1['RestingBP'],color='DarkSlateGray')\nplt.subplot(5,2,5)\nsns.distplot(heart1['Cholesterol'],color='Green')\nplt.subplot(5,2,6)\nsns.boxplot(heart1['Cholesterol'],color='Green')\nplt.subplot(5,2,7)\nsns.distplot(heart1['MaxHR'],color='Red')\nplt.subplot(5,2,8)\nsns.boxplot(heart1['MaxHR'],color='Red')\nplt.subplot(5,2,9)\nsns.distplot(heart1['Oldpeak'],color='Brown')\nplt.subplot(5,2,10)\nsns.boxplot(heart1['Oldpeak'],color='Brown')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n#assigning values to features as X and target as y\nX=heart1.drop([\"HeartDisease\"],axis=1)\ny=heart1[\"HeartDisease\"]\n\n#Set up a standard scaler for the features\ncol_names = list(X.columns)\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in X.columns:\n    if not pd.api.types.is_numeric_dtype(X[col]):\n        X[col] = le.fit_transform(X[col])\n        \ns_scaler = StandardScaler()\nX_df= s_scaler.fit_transform(X)\nX_df = pd.DataFrame(X_df, columns=col_names)   \nX_df.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nAge\n918.0\n-1.08e-16\n1.0\n-2.71\n-0.69\n0.05\n0.69\n2.49\n\n\nSex\n918.0\n-7.74e-18\n1.0\n-1.94\n0.52\n0.52\n0.52\n0.52\n\n\nChestPainType\n918.0\n1.55e-17\n1.0\n-0.82\n-0.82\n-0.82\n1.28\n2.32\n\n\nRestingBP\n918.0\n1.95e-16\n1.0\n-7.15\n-0.67\n-0.13\n0.41\n3.65\n\n\nCholesterol\n918.0\n0.00e+00\n1.0\n-1.82\n-0.23\n0.22\n0.62\n3.70\n\n\nFastingBS\n918.0\n-3.10e-17\n1.0\n-0.55\n-0.55\n-0.55\n-0.55\n1.81\n\n\nRestingECG\n918.0\n9.29e-17\n1.0\n-1.57\n0.02\n0.02\n0.02\n1.60\n\n\nMaxHR\n918.0\n4.95e-16\n1.0\n-3.02\n-0.66\n0.05\n0.75\n2.56\n\n\nExerciseAngina\n918.0\n-3.87e-18\n1.0\n-0.82\n-0.82\n-0.82\n1.21\n1.21\n\n\nOldpeak\n918.0\n1.24e-16\n1.0\n-3.27\n-0.83\n-0.27\n0.57\n4.98\n\n\nST_Slope\n918.0\n7.74e-17\n1.0\n-2.24\n-0.60\n-0.60\n1.05\n1.05\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_df,y,test_size=0.2,random_state=21)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)\n\n\nKNN Classifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nplt.rcParams['figure.figsize'] = (8,6)\nplt.rcParams['font.size'] = 20\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred_log = knn.predict(X_test)\n\nlog_train = round(knn.score(X_train, y_train) * 100, 2)\nlog_accuracy = round(accuracy_score(y_pred_log, y_test) * 100, 2)\nlog_f1 = round(f1_score(y_pred_log, y_test) * 100, 2)\n\nprint(\"Training Accuracy    :\",log_train,\"%\")\nprint(\"Model Accuracy Score :\",log_accuracy,\"%\")\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(\"Classification_Report: \\n\",classification_report(y_test,y_pred_log))\n\nTraining Accuracy    : 88.15 %\nModel Accuracy Score : 86.96 %\n--------------------------------------------------------\nClassification_Report: \n               precision    recall  f1-score   support\n\n           0       0.87      0.80      0.84        76\n           1       0.87      0.92      0.89       108\n\n    accuracy                           0.87       184\n   macro avg       0.87      0.86      0.86       184\nweighted avg       0.87      0.87      0.87       184\n\n\n\n\n\n\nConfusion Matrix (src: https://www.datacamp.com/tutorial/precision-recall-curve-tutorial)\n\n\n\nconfusion_matrix(y_test, y_pred_log);\n\n\nknn_probs = knn.predict_proba(X_test)\nknn_probs = knn_probs[:, 1]\nfrom sklearn.metrics import roc_curve, roc_auc_score\nrf_auc = roc_auc_score(y_test, knn_probs)\nrf_fpr, rf_tpr, _ = roc_curve(y_test, knn_probs)\n\n\nplt.figure(figsize=(12, 8))\nplt.plot(rf_fpr, rf_tpr, linestyle='--', label='AUC = %0.3f' % rf_auc)\n\n\nplt.title('ROC Plot')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()  \nplt.show()\n\n\n\n\n\nprecision, recall, thresholds = precision_recall_curve(y_test, knn_probs)\nplt.fill_between(recall, precision)\nplt.ylabel(\"Precision\")\nplt.xlabel(\"Recall\")\nplt.title(\"Precision-Recall curve\");"
  },
  {
    "objectID": "posts/about.html",
    "href": "posts/about.html",
    "title": "About",
    "section": "",
    "text": "I am Tahmina Sultana Priya, a PhD student in the Computer Science department at Virginia Tech, where I am passionately delving into the fascinating realm of Artificial Intelligence in the context of digital health. This field holds immense promise for revolutionizing the healthcare sector, and my research is dedicated to contributing to this exciting area. I earned my undergraduate degree in Computer Science and Engineering from Khulna University of Engineering & Technology in Bangladesh in 2020, securing the 7th position among 120 students. Following my graduation, I took on a role as a lecturer at Daffodil International University, a private educational institution in Bangladesh. During my tenure as a faculty member, I had the privilege of teaching various courses such as Data Mining, Machine Learning, Artificial Intelligence, Introduction to Bioinformatics, and Big Data, which allowed me to share my knowledge and passion with the next generation of computer scientists. In addition to my academic responsibilities, I also worked as a part-time Senior DevOps Engineer for Vertical Innovations Limited, gaining practical experience in the field of DevOps. My journey in both academia and industry has provided me with a well-rounded foundation to pursue my current research interests in the fascinating intersection of AI and digital health. The machine learning course holds significant importance in my academic journey as it equips me with the fundamental knowledge and concepts that are directly applicable to my ongoing research in the field of Artificial Intelligence and digital health. Understanding the intricacies of machine learning is not only a valuable asset for my academic pursuits but is also highly relevant to the core of my research work. It provides me with the necessary tools and insights to explore innovative ways in which machine learning can be leveraged to advance the frontiers of healthcare technology and contribute to the betterment of the healthcare sector.\n---\ntitle: \"Beatriz Milz\"\ncomments: false\nimage: bea.jpg\nabout:\n  template: trestles\n  links:\n    - icon: envelope\n      text: Email\n      href: mailto:milz.bea@gmail.com\n    - icon: file-pdf\n      text: Resume\n      href: https://beatrizmilz.github.io/resume/index.pdf  \n    - icon: github\n      text: GitHub\n      href: https://github.com/beatrizmilz\n    - icon: twitter\n      text: Twitter\n      href: https://twitter.com/BeaMilz      \n---\n&lt;center&gt;\n::: {.btn-group role=\"group\" aria-label=\"About\"}\n&lt;a href=\"about.html\"&gt; &lt;button type=\"button\" class=\"btn btn-primary me-md-2 btn-sm\"&gt;English&lt;/button&gt;&lt;/a&gt; &lt;a href=\"about-pt.html\"&gt; &lt;button type=\"button\" class=\"btn btn-primary me-md-2 btn-sm\"&gt;Português&lt;/button&gt;&lt;/a&gt;   &lt;a href=\"about-es.html\"&gt;&lt;button type=\"button\" class=\"btn btn-primary me-md-2 btn-sm\"&gt;Español&lt;/button&gt;&lt;/a&gt;\n:::\n&lt;/center&gt;\n::: {#hero-heading}\nHi!\nMy name is Beatriz, and I am a R-Lady from Brazil 🇧🇷 💜.\nI'm a [tidyverse instructor certified by RStudio](https://education.rstudio.com/trainers/people/milz+beatriz/), and I teach R at [Curso-R](https://curso-r.com/).\nI have been writing about R in my blog since 2019.\nI'm mostly active in the Brazilian R community, co-organizing communities and events, such as: [R-Ladies São Paulo](https://twitter.com/RLadiesSaoPaulo), [Latin-R](https://twitter.com/LatinR_Conf), and [satRday São Paulo](https://twitter.com/satRdaySP).\nI'm a PhD Candidate at the [University of São Paulo](http://www.iee.usp.br/), in Environmental Sciences.\nMy research is about **transparency of information** in the **management of water resources** that are used to supply the Macrometropolis of Sao Paulo.\n:::"
  },
  {
    "objectID": "posts/outlier-detection/index.html",
    "href": "posts/outlier-detection/index.html",
    "title": "Anomaly/Outlier detection",
    "section": "",
    "text": "Contents:\n\nIntroduction to Anomaly or Outlier Detection.\nExample of Anomaly or Outlier Detection manually created data.\nExample of Anomaly or Outlier Detection with real data Credit Card dataset.\nData Visualization\nData processing\nAnomaly Detection\nClustering Model implementation\nEvaluation metrics implementation\n\n\nAnomaly or Outlier Detection\nAnomaly Detection involves identifying uncommon occurrences within a dataset that stand out as statistically different from the majority of observations. These anomalies often indicate potential problems such as credit card fraud, server malfunctions, or cyber attacks.\nAnomalies fall into three primary categories:\n\nPoint Anomaly: This occurs when a data point significantly deviates from the rest of the dataset.\nContextual Anomaly: An observation is flagged as a Contextual Anomaly due to its abnormality within a specific context.\nCollective Anomaly: This involves a group of data instances that collectively indicate an anomaly.\n\nMachine Learning concepts are employed to perform anomaly detection using various approaches:\n\nSupervised Anomaly Detection: This method relies on labeled datasets containing both normal and anomalous samples. Predictive models, such as supervised Neural Networks, Support Vector Machines, or K-Nearest Neighbors Classifiers, are utilized to classify future data points.\nUnsupervised Anomaly Detection: This approach doesn’t require labeled training data. It operates on the assumptions that only a small fraction of the data is anomalous and that anomalies significantly differ from normal samples. Unsupervised methods cluster data based on similarity measures, identifying data points that fall far from the established clusters as anomalies.\n\nAnomaly Detection with manually generated data:\n\nimport numpy as np \nfrom scipy import stats \nimport matplotlib.pyplot as plt \nimport matplotlib.font_manager \nfrom pyod.models.knn import KNN  \nfrom pyod.utils.data import generate_data, get_outliers_inliers \n\n\n# generating a random dataset with two features \nX_train, y_train = generate_data(n_train = 300, train_only = True, \n                                                   n_features = 2) \n  \n# Setting the percentage of outliers \noutlier_fraction = 0.1\n  \n# Storing the outliers and inliners in different numpy arrays \nX_outliers, X_inliers = get_outliers_inliers(X_train, y_train) \nn_inliers = len(X_inliers) \nn_outliers = len(X_outliers) \n  \n# Separating the two features \nf1 = X_train[:, [0]].reshape(-1, 1) \nf2 = X_train[:, [1]].reshape(-1, 1) \n\n\nxx, yy = np.meshgrid(np.linspace(-10, 10, 200), \n                     np.linspace(-10, 10, 200)) \n  \n# scatter plot \nplt.scatter(f1, f2) \nplt.xlabel('Feature 1') \nplt.ylabel('Feature 2') \n\nText(0, 0.5, 'Feature 2')\n\n\n\n\n\n\nclf = KNN(contamination = outlier_fraction) \nclf.fit(X_train, y_train) \n  \n# You can print this to see all the prediction scores \nscores_pred = clf.decision_function(X_train)*-1\n  \ny_pred = clf.predict(X_train) \nn_errors = (y_pred != y_train).sum() \n# Counting the number of errors \n  \nprint('The number of prediction errors are ' + str(n_errors)) \n\nThe number of prediction errors are 8\n\n\n/home/tpriya/CS5525/MLBlog/env/lib/python3.10/site-packages/pyod/models/base.py:430: UserWarning: y should not be presented in unsupervised learning.\n  warnings.warn(\n\n\n\nthreshold = stats.scoreatpercentile(scores_pred, 100 * outlier_fraction) \n  \n# decision function calculates the raw  \n# anomaly score for every point \nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1\nZ = Z.reshape(xx.shape) \n  \n# fill blue colormap from minimum anomaly \n# score to threshold value \nsubplot = plt.subplot(1, 2, 1) \nsubplot.contourf(xx, yy, Z, levels = np.linspace(Z.min(),  \n                  threshold, 10), cmap = plt.cm.Blues_r) \n  \n# draw red contour line where anomaly  \n# score is equal to threshold \na = subplot.contour(xx, yy, Z, levels =[threshold], \n                     linewidths = 2, colors ='red') \n  \n# fill orange contour lines where range of anomaly \n# score is from threshold to maximum anomaly score \nsubplot.contourf(xx, yy, Z, levels =[threshold, Z.max()], colors ='orange') \n  \n# scatter plot of inliers with white dots \nb = subplot.scatter(X_train[:-n_outliers, 0], X_train[:-n_outliers, 1], \n                                    c ='white', s = 20, edgecolor ='k')  \n  \n# scatter plot of outliers with black dots \nc = subplot.scatter(X_train[-n_outliers:, 0], X_train[-n_outliers:, 1],  \n                                    c ='black', s = 20, edgecolor ='k') \nsubplot.axis('tight') \n  \nsubplot.legend( \n    [a.collections[0], b, c], \n    ['learned decision function', 'true inliers', 'true outliers'], \n    prop = matplotlib.font_manager.FontProperties(size = 10), \n    loc ='lower right') \n  \nsubplot.set_title('K-Nearest Neighbours') \nsubplot.set_xlim((-10, 10)) \nsubplot.set_ylim((-10, 10)) \nplt.show()  \n\n/tmp/ipykernel_25173/1166639198.py:33: MatplotlibDeprecationWarning: The collections attribute was deprecated in Matplotlib 3.8 and will be removed two minor releases later.\n  [a.collections[0], b, c],\n\n\n\n\n\nAnomaly Detection with real data:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.ensemble import IsolationForest\n\n\nfrom sklearn.metrics import silhouette_score\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/outlier-detection/CC GENERAL.csv')\n\nprint('The shape of the dataset is:', df.shape)\n\nThe shape of the dataset is: (8950, 18)\n\n\n\n# check number of nulls in each column \ndf.isnull().sum().sort_values(ascending=False)\n# konw the ratio of null in each column \nround(df.isnull().sum(axis=0)*100/df.shape[0],2).sort_values(ascending=False)\n\nMINIMUM_PAYMENTS                    3.50\nCREDIT_LIMIT                        0.01\nCUST_ID                             0.00\nBALANCE                             0.00\nPRC_FULL_PAYMENT                    0.00\nPAYMENTS                            0.00\nPURCHASES_TRX                       0.00\nCASH_ADVANCE_TRX                    0.00\nCASH_ADVANCE_FREQUENCY              0.00\nPURCHASES_INSTALLMENTS_FREQUENCY    0.00\nONEOFF_PURCHASES_FREQUENCY          0.00\nPURCHASES_FREQUENCY                 0.00\nCASH_ADVANCE                        0.00\nINSTALLMENTS_PURCHASES              0.00\nONEOFF_PURCHASES                    0.00\nPURCHASES                           0.00\nBALANCE_FREQUENCY                   0.00\nTENURE                              0.00\ndtype: float64\n\n\n\n# save numeric columns and objects in separeted list to handle each one of them\nnumeric_columns = df.select_dtypes(exclude=['object']).columns.to_list() \nobject_columns = df.select_dtypes(include=['object']).columns.to_list()\n\n\ndf[numeric_columns].hist(bins=15, figsize=(20,15))\n\narray([[&lt;Axes: title={'center': 'BALANCE'}&gt;,\n        &lt;Axes: title={'center': 'BALANCE_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES'}&gt;,\n        &lt;Axes: title={'center': 'ONEOFF_PURCHASES'}&gt;],\n       [&lt;Axes: title={'center': 'INSTALLMENTS_PURCHASES'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'ONEOFF_PURCHASES_FREQUENCY'}&gt;],\n       [&lt;Axes: title={'center': 'PURCHASES_INSTALLMENTS_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE_TRX'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES_TRX'}&gt;],\n       [&lt;Axes: title={'center': 'CREDIT_LIMIT'}&gt;,\n        &lt;Axes: title={'center': 'PAYMENTS'}&gt;,\n        &lt;Axes: title={'center': 'MINIMUM_PAYMENTS'}&gt;,\n        &lt;Axes: title={'center': 'PRC_FULL_PAYMENT'}&gt;],\n       [&lt;Axes: title={'center': 'TENURE'}&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]],\n      dtype=object)\n\n\n\n\n\nOutliers:\n\nplt.subplots(figsize=(15, 15))\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)\n\nfor i, column in enumerate(numeric_columns, 1):\n    plt.subplot(7, 3, i)\n    sns.boxplot(df[column], orient='h')\n\n\n\n\nData Preprocessing:\n\ndf_pre=df.copy()\ndf_pre.drop(object_columns, axis=1, inplace=True)\ndf_pre.head(3)\ncolumns_names = list(df_pre.columns)\ncolumns_names\n\n['BALANCE',\n 'BALANCE_FREQUENCY',\n 'PURCHASES',\n 'ONEOFF_PURCHASES',\n 'INSTALLMENTS_PURCHASES',\n 'CASH_ADVANCE',\n 'PURCHASES_FREQUENCY',\n 'ONEOFF_PURCHASES_FREQUENCY',\n 'PURCHASES_INSTALLMENTS_FREQUENCY',\n 'CASH_ADVANCE_FREQUENCY',\n 'CASH_ADVANCE_TRX',\n 'PURCHASES_TRX',\n 'CREDIT_LIMIT',\n 'PAYMENTS',\n 'MINIMUM_PAYMENTS',\n 'PRC_FULL_PAYMENT',\n 'TENURE']\n\n\n\nfrom sklearn.impute import SimpleImputer\ndf_NoNull = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(df_pre), columns=columns_names)\ndf_NoNull \n\n\n\n\n\n\n\n\nBALANCE\nBALANCE_FREQUENCY\nPURCHASES\nONEOFF_PURCHASES\nINSTALLMENTS_PURCHASES\nCASH_ADVANCE\nPURCHASES_FREQUENCY\nONEOFF_PURCHASES_FREQUENCY\nPURCHASES_INSTALLMENTS_FREQUENCY\nCASH_ADVANCE_FREQUENCY\nCASH_ADVANCE_TRX\nPURCHASES_TRX\nCREDIT_LIMIT\nPAYMENTS\nMINIMUM_PAYMENTS\nPRC_FULL_PAYMENT\nTENURE\n\n\n\n\n0\n40.900749\n0.818182\n95.40\n0.00\n95.40\n0.000000\n0.166667\n0.000000\n0.083333\n0.000000\n0.0\n2.0\n1000.0\n201.802084\n139.509787\n0.000000\n12.0\n\n\n1\n3202.467416\n0.909091\n0.00\n0.00\n0.00\n6442.945483\n0.000000\n0.000000\n0.000000\n0.250000\n4.0\n0.0\n7000.0\n4103.032597\n1072.340217\n0.222222\n12.0\n\n\n2\n2495.148862\n1.000000\n773.17\n773.17\n0.00\n0.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.0\n12.0\n7500.0\n622.066742\n627.284787\n0.000000\n12.0\n\n\n3\n1666.670542\n0.636364\n1499.00\n1499.00\n0.00\n205.788017\n0.083333\n0.083333\n0.000000\n0.083333\n1.0\n1.0\n7500.0\n0.000000\n312.343947\n0.000000\n12.0\n\n\n4\n817.714335\n1.000000\n16.00\n16.00\n0.00\n0.000000\n0.083333\n0.083333\n0.000000\n0.000000\n0.0\n1.0\n1200.0\n678.334763\n244.791237\n0.000000\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8945\n28.493517\n1.000000\n291.12\n0.00\n291.12\n0.000000\n1.000000\n0.000000\n0.833333\n0.000000\n0.0\n6.0\n1000.0\n325.594462\n48.886365\n0.500000\n6.0\n\n\n8946\n19.183215\n1.000000\n300.00\n0.00\n300.00\n0.000000\n1.000000\n0.000000\n0.833333\n0.000000\n0.0\n6.0\n1000.0\n275.861322\n312.343947\n0.000000\n6.0\n\n\n8947\n23.398673\n0.833333\n144.40\n0.00\n144.40\n0.000000\n0.833333\n0.000000\n0.666667\n0.000000\n0.0\n5.0\n1000.0\n81.270775\n82.418369\n0.250000\n6.0\n\n\n8948\n13.457564\n0.833333\n0.00\n0.00\n0.00\n36.558778\n0.000000\n0.000000\n0.000000\n0.166667\n2.0\n0.0\n500.0\n52.549959\n55.755628\n0.250000\n6.0\n\n\n8949\n372.708075\n0.666667\n1093.25\n1093.25\n0.00\n127.040008\n0.666667\n0.666667\n0.000000\n0.333333\n2.0\n23.0\n1200.0\n63.165404\n88.288956\n0.000000\n6.0\n\n\n\n\n8950 rows × 17 columns\n\n\n\nLog Transform for handling outliers:\n\n# will add 1 to all values because log transform get error for numbers between 0 and 1\ndf_pre2 = (df_NoNull + 1) \ndf_log = np.log(df_pre2)\ndf_log.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nBALANCE\n8950.0\n6.161637\n2.013303\n0.000000\n4.861995\n6.773521\n7.628099\n9.854515\n\n\nBALANCE_FREQUENCY\n8950.0\n0.619940\n0.148590\n0.000000\n0.635989\n0.693147\n0.693147\n0.693147\n\n\nPURCHASES\n8950.0\n4.899647\n2.916872\n0.000000\n3.704627\n5.892417\n7.013133\n10.800403\n\n\nONEOFF_PURCHASES\n8950.0\n3.204274\n3.246365\n0.000000\n0.000000\n3.663562\n6.360274\n10.615512\n\n\nINSTALLMENTS_PURCHASES\n8950.0\n3.352403\n3.082973\n0.000000\n0.000000\n4.499810\n6.151961\n10.021315\n\n\nCASH_ADVANCE\n8950.0\n3.319086\n3.566298\n0.000000\n0.000000\n0.000000\n7.016449\n10.760839\n\n\nPURCHASES_FREQUENCY\n8950.0\n0.361268\n0.277317\n0.000000\n0.080042\n0.405465\n0.650588\n0.693147\n\n\nONEOFF_PURCHASES_FREQUENCY\n8950.0\n0.158699\n0.216672\n0.000000\n0.000000\n0.080042\n0.262364\n0.693147\n\n\nPURCHASES_INSTALLMENTS_FREQUENCY\n8950.0\n0.270072\n0.281852\n0.000000\n0.000000\n0.154151\n0.559616\n0.693147\n\n\nCASH_ADVANCE_FREQUENCY\n8950.0\n0.113512\n0.156716\n0.000000\n0.000000\n0.000000\n0.200671\n0.916291\n\n\nCASH_ADVANCE_TRX\n8950.0\n0.817570\n1.009316\n0.000000\n0.000000\n0.000000\n1.609438\n4.820282\n\n\nPURCHASES_TRX\n8950.0\n1.894731\n1.373856\n0.000000\n0.693147\n2.079442\n2.890372\n5.883322\n\n\nCREDIT_LIMIT\n8950.0\n8.094825\n0.819629\n3.931826\n7.378384\n8.006701\n8.779711\n10.308986\n\n\nPAYMENTS\n8950.0\n6.624540\n1.591763\n0.000000\n5.951361\n6.754489\n7.550732\n10.834125\n\n\nMINIMUM_PAYMENTS\n8950.0\n5.916079\n1.169929\n0.018982\n5.146667\n5.747301\n6.671670\n11.243832\n\n\nPRC_FULL_PAYMENT\n8950.0\n0.117730\n0.211617\n0.000000\n0.000000\n0.000000\n0.133531\n0.693147\n\n\nTENURE\n8950.0\n2.519680\n0.130367\n1.945910\n2.564949\n2.564949\n2.564949\n2.564949\n\n\n\n\n\n\n\n\nf, axs = plt.subplots(figsize=(15, 15))\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)\n\nfor i, column in enumerate(df_log.columns, 1):\n    plt.subplot(7, 3, i)\n    sns.boxplot(df_log[column], orient='h')\n\n\n\n\n\ndf_pre2 = df_NoNull.copy()\n\n\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer \ndf_power = PowerTransformer(method=\"yeo-johnson\").fit_transform(df_pre2)\n\n\ndf_power= pd.DataFrame(df_power, columns=columns_names)\ndf_power.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nBALANCE\n8950.0\n-4.445854e-17\n1.000056\n-2.120305\n-0.816807\n0.151681\n0.717913\n2.731227\n\n\nBALANCE_FREQUENCY\n8950.0\n-4.699903e-16\n1.000056\n-1.997716\n-0.598992\n0.628612\n0.628612\n0.628612\n\n\nPURCHASES\n8950.0\n1.238488e-16\n1.000056\n-1.505149\n-0.654799\n0.160770\n0.710251\n3.559070\n\n\nONEOFF_PURCHASES\n8950.0\n6.986342e-17\n1.000056\n-1.000488\n-1.000488\n0.237516\n0.990945\n1.959134\n\n\nINSTALLMENTS_PURCHASES\n8950.0\n7.343598e-17\n1.000056\n-1.085422\n-1.085422\n0.361196\n0.906100\n2.212015\n\n\nCASH_ADVANCE\n8950.0\n1.381390e-16\n1.000056\n-0.944538\n-0.944538\n-0.944538\n1.059052\n1.729923\n\n\nPURCHASES_FREQUENCY\n8950.0\n6.668781e-17\n1.000056\n-1.278866\n-1.015926\n0.119657\n1.050283\n1.218860\n\n\nONEOFF_PURCHASES_FREQUENCY\n8950.0\n-2.540488e-17\n1.000056\n-0.903315\n-0.903315\n-0.092230\n0.978077\n1.732554\n\n\nPURCHASES_INSTALLMENTS_FREQUENCY\n8950.0\n-7.462683e-17\n1.000056\n-1.004445\n-1.004445\n-0.273845\n1.069884\n1.377136\n\n\nCASH_ADVANCE_FREQUENCY\n8950.0\n-9.804696e-17\n1.000056\n-0.883204\n-0.883204\n-0.883204\n1.016910\n1.902718\n\n\nCASH_ADVANCE_TRX\n8950.0\n-1.254366e-16\n1.000056\n-0.905801\n-0.905801\n-0.905801\n1.044342\n1.921254\n\n\nPURCHASES_TRX\n8950.0\n-2.889805e-16\n1.000056\n-1.387624\n-0.872718\n0.143257\n0.729118\n2.838620\n\n\nCREDIT_LIMIT\n8950.0\n-4.826927e-16\n1.000056\n-4.573150\n-0.880590\n-0.129660\n0.830085\n2.851804\n\n\nPAYMENTS\n8950.0\n-5.716098e-17\n1.000056\n-2.782085\n-0.608905\n-0.064284\n0.569453\n4.568553\n\n\nMINIMUM_PAYMENTS\n8950.0\n-6.668781e-16\n1.000056\n-5.869902\n-0.643745\n-0.115812\n0.665890\n4.036562\n\n\nPRC_FULL_PAYMENT\n8950.0\n-6.986342e-17\n1.000056\n-0.677889\n-0.677889\n-0.677889\n0.854117\n1.873638\n\n\nTENURE\n8950.0\n7.621464e-16\n1.000056\n-2.526612\n0.422252\n0.422252\n0.422252\n0.422252\n\n\n\n\n\n\n\n\nf, axs = plt.subplots(figsize=(15, 15))\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)\n\nfor i, column in enumerate(df_power.columns, 1):\n    plt.subplot(7, 3, i)\n    sns.boxplot(df_power[column], orient='h')\n\n\n\n\n\ndf_power.hist(bins=20, figsize=(20,15))\n\narray([[&lt;Axes: title={'center': 'BALANCE'}&gt;,\n        &lt;Axes: title={'center': 'BALANCE_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES'}&gt;,\n        &lt;Axes: title={'center': 'ONEOFF_PURCHASES'}&gt;],\n       [&lt;Axes: title={'center': 'INSTALLMENTS_PURCHASES'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'ONEOFF_PURCHASES_FREQUENCY'}&gt;],\n       [&lt;Axes: title={'center': 'PURCHASES_INSTALLMENTS_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE_FREQUENCY'}&gt;,\n        &lt;Axes: title={'center': 'CASH_ADVANCE_TRX'}&gt;,\n        &lt;Axes: title={'center': 'PURCHASES_TRX'}&gt;],\n       [&lt;Axes: title={'center': 'CREDIT_LIMIT'}&gt;,\n        &lt;Axes: title={'center': 'PAYMENTS'}&gt;,\n        &lt;Axes: title={'center': 'MINIMUM_PAYMENTS'}&gt;,\n        &lt;Axes: title={'center': 'PRC_FULL_PAYMENT'}&gt;],\n       [&lt;Axes: title={'center': 'TENURE'}&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]],\n      dtype=object)\n\n\n\n\n\nFeature Transform\n\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n\nscale_MinMax = MinMaxScaler()\ndf_transformed = pd.DataFrame(scale_MinMax.fit_transform(df_NoNull), columns=columns_names)\nscale_MinMax = MinMaxScaler()\ndf_transformed_Log = pd.DataFrame(scale_MinMax.fit_transform(df_log), columns=columns_names)\nscale_MinMax = MinMaxScaler()\ndf_transformed_Power = pd.DataFrame(scale_MinMax.fit_transform(df_power), columns=columns_names)\nscale_Standard = StandardScaler()\ndf_transformed_Power = pd.DataFrame(scale_Standard.fit_transform(df_power), columns=columns_names)\ndf_log.to_csv(\"./Data_Log.csv\",index=False)\n\n\n\nData with Log Transformation to Clustering and Anomaly detection\n\ndf_transformed = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/outlier-detection/Data_Log.csv')\ndf_transformed\n\n\n\n\n\n\n\n\nBALANCE\nBALANCE_FREQUENCY\nPURCHASES\nONEOFF_PURCHASES\nINSTALLMENTS_PURCHASES\nCASH_ADVANCE\nPURCHASES_FREQUENCY\nONEOFF_PURCHASES_FREQUENCY\nPURCHASES_INSTALLMENTS_FREQUENCY\nCASH_ADVANCE_FREQUENCY\nCASH_ADVANCE_TRX\nPURCHASES_TRX\nCREDIT_LIMIT\nPAYMENTS\nMINIMUM_PAYMENTS\nPRC_FULL_PAYMENT\nTENURE\n\n\n\n\n0\n3.735304\n0.597837\n4.568506\n0.000000\n4.568506\n0.000000\n0.154151\n0.000000\n0.080042\n0.000000\n0.000000\n1.098612\n6.908755\n5.312231\n4.945277\n0.000000\n2.564949\n\n\n1\n8.071989\n0.646627\n0.000000\n0.000000\n0.000000\n8.770896\n0.000000\n0.000000\n0.000000\n0.223144\n1.609438\n0.000000\n8.853808\n8.319725\n6.978531\n0.200671\n2.564949\n\n\n2\n7.822504\n0.693147\n6.651791\n6.651791\n0.000000\n0.000000\n0.693147\n0.693147\n0.000000\n0.000000\n0.000000\n2.564949\n8.922792\n6.434654\n6.442994\n0.000000\n2.564949\n\n\n3\n7.419183\n0.492477\n7.313220\n7.313220\n0.000000\n5.331694\n0.080042\n0.080042\n0.000000\n0.080042\n0.693147\n0.693147\n8.922792\n0.000000\n5.747301\n0.000000\n2.564949\n\n\n4\n6.707735\n0.693147\n2.833213\n2.833213\n0.000000\n0.000000\n0.080042\n0.080042\n0.000000\n0.000000\n0.000000\n0.693147\n7.090910\n6.521114\n5.504483\n0.000000\n2.564949\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8945\n3.384170\n0.693147\n5.677165\n0.000000\n5.677165\n0.000000\n0.693147\n0.000000\n0.606136\n0.000000\n0.000000\n1.945910\n6.908755\n5.788719\n3.909748\n0.405465\n1.945910\n\n\n8946\n3.004851\n0.693147\n5.707110\n0.000000\n5.707110\n0.000000\n0.693147\n0.000000\n0.606136\n0.000000\n0.000000\n1.945910\n6.908755\n5.623517\n5.747301\n0.000000\n1.945910\n\n\n8947\n3.194529\n0.606136\n4.979489\n0.000000\n4.979489\n0.000000\n0.606136\n0.000000\n0.510826\n0.000000\n0.000000\n1.791759\n6.908755\n4.410016\n4.423869\n0.223144\n1.945910\n\n\n8948\n2.671218\n0.606136\n0.000000\n0.000000\n0.000000\n3.625907\n0.000000\n0.000000\n0.000000\n0.154151\n1.098612\n0.000000\n6.216606\n3.980615\n4.038755\n0.223144\n1.945910\n\n\n8949\n5.923475\n0.510826\n6.997824\n6.997824\n0.000000\n4.852343\n0.510826\n0.510826\n0.000000\n0.287682\n1.098612\n3.178054\n7.090910\n4.161464\n4.491878\n0.000000\n1.945910\n\n\n\n\n8950 rows × 17 columns\n\n\n\nClustering\n\n# To plot Elbow With Inertia \ninertia = []\nRange = [*range(1,11)]\n\nfor k in Range: \n    kmean = KMeans(n_clusters=k, max_iter=300, random_state=42)\n    kmean.fit(df_transformed)\n    inertia.append(kmean.inertia_)\n    \nplt.figure(figsize=(10,4))\nplt.plot(Range, inertia, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method ')\nplt.show()\n\n\n\n\n\ndrop_variation = []\ndrop_variation.append(0) #add 0 in the first element \n\nfor i in range(len(inertia) -1):\n    dropValue = inertia[i] - inertia[i+1]\n    drop_variation.append(dropValue) \n\n# select suitable k that have large drop in the variation\nk = Range[np.argmax(drop_variation)]\nprint(\"Suitable number of clusters = \",k)\n\nSuitable number of clusters =  2\n\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=4, random_state=42).fit(df_transformed)\n\nlabels = kmeans.labels_\niner = kmeans.inertia_\ncent = kmeans.cluster_centers_\n\nprint(\"\\t~~ THIS RESULT OF K-mean SKLEARN  ~~\")\nprint('~'*50)\nprint(\"sum of elements that contain in cluster 0 :\",(labels == 0).sum())\nprint(\"sum of elements that contain in cluster 1 :\",(labels == 1).sum())\nprint(\"sum of elements that contain in cluster 2 :\",(labels == 2).sum())\nprint(\"sum of elements that contain in cluster 3 :\",(labels == 3).sum())\n\n    ~~ THIS RESULT OF K-mean SKLEARN  ~~\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nsum of elements that contain in cluster 0 : 2086\nsum of elements that contain in cluster 1 : 2133\nsum of elements that contain in cluster 2 : 1976\nsum of elements that contain in cluster 3 : 2755\n\n\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.metrics import silhouette_score\n\n\nscore = silhouette_score(df_transformed,  labels, metric='euclidean')\nprint('Silhouett Score: %.3f' % score)\nvisualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n\nvisualizer.fit(df_transformed)\nvisualizer.show()  \n\nSilhouett Score: 0.400\n\n\n\n\n\n&lt;Axes: title={'center': 'Silhouette Plot of KMeans Clustering for 8950 Samples in 4 Centers'}, xlabel='silhouette coefficient values', ylabel='cluster label'&gt;\n\n\n\nfrom sklearn.decomposition import KernelPCA\nkpca = KernelPCA(n_components=10, kernel='rbf')\ndf_kpca = pd.DataFrame(kpca.fit_transform(df_transformed))\ndf_kpca\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n-0.322024\n0.516047\n0.085217\n0.036215\n-0.015608\n-0.013188\n-0.345512\n0.052362\n0.067164\n0.204233\n\n\n1\n0.571038\n0.078448\n0.114536\n0.005173\n-0.033539\n0.586146\n-0.004310\n0.012051\n0.002466\n0.001755\n\n\n2\n-0.097995\n-0.181926\n-0.298064\n0.500376\n-0.085155\n0.017446\n0.030665\n0.070862\n-0.147242\n0.267388\n\n\n3\n-0.031012\n-0.038757\n-0.127563\n-0.017882\n0.049901\n-0.002018\n0.004615\n0.049781\n0.032890\n-0.013316\n\n\n4\n-0.042195\n-0.045667\n-0.203879\n0.215753\n-0.010849\n-0.072370\n0.016403\n0.047356\n0.017663\n0.074450\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8945\n-0.376092\n0.612240\n0.150075\n0.016713\n-0.014451\n0.003618\n-0.282473\n0.008226\n-0.007720\n0.018540\n\n\n8946\n-0.373469\n0.605441\n0.145621\n0.015209\n-0.013303\n0.002802\n-0.247563\n0.017263\n-0.006531\n0.050644\n\n\n8947\n-0.326389\n0.532047\n0.093867\n0.027570\n-0.015974\n-0.005210\n-0.444072\n0.050256\n0.070342\n0.165628\n\n\n8948\n0.052839\n-0.013372\n-0.091298\n-0.008970\n0.022265\n-0.223196\n-0.010937\n0.064317\n0.052705\n-0.028263\n\n\n8949\n-0.046451\n-0.076523\n-0.235957\n0.025098\n0.191674\n0.006375\n-0.012778\n-0.017814\n-0.028892\n-0.009289\n\n\n\n\n8950 rows × 10 columns\n\n\n\n\nfrom sklearn.ensemble import IsolationForest\nclf = IsolationForest(random_state=0, \n                      max_features=2,\n                      n_estimators=100,\n                      contamination=0.1).fit(df_kpca)\n\nanom_pred = clf.predict(df_kpca)\nanom_pred\n\narray([1, 1, 1, ..., 1, 1, 1])\n\n\n\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, \n            perplexity=50,  \n            random_state=42,\n            n_iter=300).fit_transform(df_transformed)\n\n\ndf_embed_Iso = pd.DataFrame(tsne, columns=['feature1', 'feature2'])\ndf_embed_Iso['Labels']= pd.DataFrame(anom_pred)\ndf_embed_Iso\n\n\n\n\n\n\n\n\nfeature1\nfeature2\nLabels\n\n\n\n\n0\n0.407689\n-5.521269\n1\n\n\n1\n-5.836224\n2.789712\n1\n\n\n2\n0.094771\n3.217455\n1\n\n\n3\n-1.684396\n6.508462\n1\n\n\n4\n3.838952\n4.256968\n1\n\n\n...\n...\n...\n...\n\n\n8945\n0.704921\n-7.746036\n1\n\n\n8946\n0.093061\n-7.773806\n1\n\n\n8947\n-0.642122\n-5.856036\n1\n\n\n8948\n-5.276070\n-6.167468\n1\n\n\n8949\n-2.545392\n7.006695\n1\n\n\n\n\n8950 rows × 3 columns\n\n\n\nTSNE Visualization\n\nplt.figure(figsize=(9,5))\nsns.scatterplot(\n    x='feature1', y='feature2',\n    data=df_embed_Iso,    \n    hue=df_embed_Iso['Labels'],\n    palette=sns.color_palette(\"hls\", 2)\n)\n\n&lt;Axes: xlabel='feature1', ylabel='feature2'&gt;"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Tahmina, and I am from Bangladesh.\nI am a PhD student in the Computer Science department at Virginia Tech, where I am passionately delving into the fascinating realm of Artificial Intelligence in the context of digital health. This field holds immense promise for revolutionizing the healthcare sector, and my research is dedicated to contributing to this exciting area.\nThe machine learning course holds significant importance in my academic journey as it equips me with the fundamental knowledge and concepts that are directly applicable to my ongoing research in the field of Artificial Intelligence and digital health. Understanding the intricacies of machine learning is not only a valuable asset for my academic pursuits but is also highly relevant to the core of my research work. It provides me with the necessary tools and insights to explore innovative ways in which machine learning can be leveraged to advance the frontiers of healthcare technology and contribute to the betterment of the healthcare sector."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tahmina Sultana Priya",
    "section": "",
    "text": "python\n\n\ncode\n\n\nanalysis\n\n\n\n\nClassification in Machine Learning involves the process of assigning predefined categories or labels to data points based on their features, enabling algorithms to learn and predict the class of new, unseen instances.\n\n\n\n\n\n\nNov 24, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nProbability in Machine Learning enables the quantification of uncertainty, allowing algorithms to make informed decisions by assessing the likelihood of various outcomes or events occurring based on available data.\n\n\n\n\n\n\nNov 22, 2023\n\n\nTahmina Sultana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nAnomaly detection in Machine Learning focuses on identifying rare or unusual instances in data that significantly differ from the majority of normal observations, aiding in the detection of outliers or irregular patterns.\n\n\n\n\n\n\nNov 20, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nLinear Regression is a supervised learning algorithm used for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n\n\n\n\n\n\nNov 5, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nNonlinear Regression involves modeling the relationship between dependent and independent variables using a nonlinear function, allowing for more complex and flexible patterns beyond linear relationships.\n\n\n\n\n\n\nOct 29, 2023\n\n\nTahmina Sultana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nClustering in Machine Learning involves grouping similar data points together based on certain features or characteristics, aiming to discover inherent patterns or structures within the data.\n\n\n\n\n\n\nOct 28, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Tahmina Sultana Priya",
    "section": "",
    "text": "python\n\n\ncode\n\n\nanalysis\n\n\n\n\nClassification in Machine Learning involves the process of assigning predefined categories or labels to data points based on their features, enabling algorithms to learn and predict the class of new, unseen instances.\n\n\n\n\n\n\nNov 24, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nProbability in Machine Learning enables the quantification of uncertainty, allowing algorithms to make informed decisions by assessing the likelihood of various outcomes or events occurring based on available data.\n\n\n\n\n\n\nNov 22, 2023\n\n\nTahmina Sultana\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nAnomaly detection in Machine Learning focuses on identifying rare or unusual instances in data that significantly differ from the majority of normal observations, aiding in the detection of outliers or irregular patterns.\n\n\n\n\n\n\nNov 20, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nLinear Regression is a supervised learning algorithm used for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n\n\n\n\n\n\nNov 5, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nNonlinear Regression involves modeling the relationship between dependent and independent variables using a nonlinear function, allowing for more complex and flexible patterns beyond linear relationships.\n\n\n\n\n\n\nOct 29, 2023\n\n\nTahmina Sultana\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\n\nClustering in Machine Learning involves grouping similar data points together based on certain features or characteristics, aiming to discover inherent patterns or structures within the data.\n\n\n\n\n\n\nOct 28, 2023\n\n\nTahmina Sultana\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Image Source: https://static.javatpoint.com/tutorial/machine-learning/images/clustering-in-machine-learning.png\nContents:"
  },
  {
    "objectID": "posts/post-with-code/index.html#clustering",
    "href": "posts/post-with-code/index.html#clustering",
    "title": "Clustering",
    "section": "Clustering",
    "text": "Clustering\nClustering involves the task of grouping a population or data points into distinct clusters, where items within the same cluster share more similarities with each other compared to those in different clusters. Essentially, the goal is to identify and categorize groups with similar characteristics into clusters.\nThere are various types of clustering algorithms due to the subjective nature of clustering:\n\nConnectivity models: These models rely on the premise that closer data points in the data space exhibit greater similarity. They can start by classifying each point into separate clusters and then merge them as distance decreases, or begin with one cluster and split it as distance increases. Hierarchical clustering is a prominent example of this model.\nCentroid models: These iterative algorithms gauge similarity based on a point’s proximity to cluster centroids. K-Means clustering falls into this category, requiring the number of clusters to be specified beforehand, which demands prior knowledge of the dataset.\nDistribution models: These models assess the probability that data points in a cluster belong to the same distribution, like Normal or Gaussian. The Expectation-maximization algorithm is an instance, but it tends to overfit the data.\nDensity Models: These algorithms explore data space for areas with varying densities of data points. They identify different density regions and group data points within these regions into the same cluster. DBSCAN and OPTICS are popular examples.\n\nI’ll delve deeper into two widely used clustering algorithms: K Means and Hierarchical clustering.\n\n1. KMeans Clustering:\nK-Means clustering is an unsupervised technique used to group data without pre-existing labels for training. Instead, it relies on the inherent patterns within independent features to derive insights on unseen data.\n\n\n\nSrc: https://www.kaggle.com/code/pythonkumar/clustering-k-means-dbscan-gmm-bgmm-dendogram-viz\n\n\n\n\n2. Hierarchical Clustering\nHierarchical Clustering, also known as Hierarchical Cluster Analysis (HCA), is an unsupervised clustering method that organizes clusters with a clear top-to-bottom order.\nThis algorithm groups similar objects into clusters, resembling the hierarchical organization seen in file and folder structures on a hard disk. The primary goal is to create a set of distinct clusters, each cluster being unique from the others, while objects within each cluster share substantial similarities.\nHierarchical clustering is typically classified into two types:\nAgglomerative Hierarchical Clustering\n\n\n\nSrc: https://www.kaggle.com/code/pythonkumar/clustering-k-means-dbscan-gmm-bgmm-dendogram-viz\n\n\nDivisive Hierarchical Clustering\n\n\n\n\n\n3. Density Based(DBSCAN)\n\n\n\nSrc: https://www.kaggle.com/code/pythonkumar/clustering-k-means-dbscan-gmm-bgmm-dendogram-viz\n\n\nDBSCAN (Density-based spatial clustering of applications with noise) is an algorithm designed to identify clusters of varying shapes and sizes within a dataset, even in the presence of noise and outliers.\nThe algorithm relies on two key parameters:\n\nminPts: This threshold determines the minimum number of points required to be clustered together for a region to be recognized as dense.\neps (ε): A distance measurement used to locate neighboring points around any given point.\n\nAfter completing the DBSCAN clustering, three types of points emerge:\n\nCore: These points have at least m neighboring points within a distance of n from themselves.\nBorder: Points classified as Border have at least one Core point within a distance of n.\nNoise: These points neither qualify as Core nor Border points. They have fewer than m neighboring points within a distance of n from themselves.\n\n\n\n4. Gaussian Mixture Model\nGaussian Mixture Models (GMMs) models assume multiple Gaussian distributions, each representing a cluster. GMMs use a soft clustering approach, probabilistically assigning data points to different clusters. The algorithm comprises two steps: the Expectation (E) step and the Maximization (M) step.\n\n\n\nSrc: https://www.kaggle.com/code/pythonkumar/clustering-k-means-dbscan-gmm-bgmm-dendogram-viz"
  },
  {
    "objectID": "posts/post-with-code/index.html#kmodes-clustering-using-cardio-data",
    "href": "posts/post-with-code/index.html#kmodes-clustering-using-cardio-data",
    "title": "Clustering",
    "section": "KModes Clustering using Cardio Data",
    "text": "KModes Clustering using Cardio Data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nfrom kmodes.kmodes import KModes\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.cluster import KMeans \nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/post-with-code/cardio_train.csv', sep=\";\")\ndf\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\n\n\n\n\n0\n0\n18393\n2\n168\n62.0\n110\n80\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n20228\n1\n156\n85.0\n140\n90\n3\n1\n0\n0\n1\n1\n\n\n2\n2\n18857\n1\n165\n64.0\n130\n70\n3\n1\n0\n0\n0\n1\n\n\n3\n3\n17623\n2\n169\n82.0\n150\n100\n1\n1\n0\n0\n1\n1\n\n\n4\n4\n17474\n1\n156\n56.0\n100\n60\n1\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n69995\n99993\n19240\n2\n168\n76.0\n120\n80\n1\n1\n1\n0\n1\n0\n\n\n69996\n99995\n22601\n1\n158\n126.0\n140\n90\n2\n2\n0\n0\n1\n1\n\n\n69997\n99996\n19066\n2\n183\n105.0\n180\n90\n3\n1\n0\n1\n0\n1\n\n\n69998\n99998\n22431\n1\n163\n72.0\n135\n80\n1\n2\n0\n0\n0\n1\n\n\n69999\n99999\n20540\n1\n170\n72.0\n120\n80\n2\n1\n0\n0\n1\n0\n\n\n\n\n70000 rows × 13 columns\n\n\n\n\n#removing Outliers\n# Let's remove weights and heights, that fall below 2.5% or above 97.5% of a given range.\ndf.drop(df[(df['height'] &gt; df['height'].quantile(0.975)) | (df['height'] &lt; df['height'].quantile(0.025))].index,inplace=True)\ndf.drop(df[(df['weight'] &gt; df['weight'].quantile(0.975)) | (df['weight'] &lt; df['weight'].quantile(0.025))].index,inplace=True)\n\n\nprint(\"Diastilic pressure is higher than systolic one in {0} cases\".format(df[df['ap_lo']&gt; df['ap_hi']].shape[0]))\ndf.drop(df[(df['ap_hi'] &gt; df['ap_hi'].quantile(0.975)) | (df['ap_hi'] &lt; df['ap_hi'].quantile(0.025))].index,inplace=True)\ndf.drop(df[(df['ap_lo'] &gt; df['ap_lo'].quantile(0.975)) | (df['ap_lo'] &lt; df['ap_lo'].quantile(0.025))].index,inplace=True)\n\nDiastilic pressure is higher than systolic one in 1082 cases\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nid\nage\ngender\nheight\nweight\nap_hi\nap_lo\ncholesterol\ngluc\nsmoke\nalco\nactive\ncardio\n\n\n\n\ncount\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n60142.000000\n\n\nmean\n49895.698065\n19468.719979\n1.347311\n164.554854\n73.426805\n125.770526\n81.046307\n1.350953\n1.220229\n0.085631\n0.051877\n0.803648\n0.488228\n\n\nstd\n28840.467755\n2460.510296\n0.476120\n6.830174\n11.614806\n13.761847\n8.239157\n0.670076\n0.567607\n0.279820\n0.221781\n0.397241\n0.499866\n\n\nmin\n0.000000\n10798.000000\n1.000000\n150.000000\n52.000000\n100.000000\n60.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n24867.500000\n17677.250000\n1.000000\n160.000000\n65.000000\n120.000000\n80.000000\n1.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n50%\n49902.500000\n19705.000000\n1.000000\n165.000000\n72.000000\n120.000000\n80.000000\n1.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n75%\n74745.750000\n21321.000000\n2.000000\n169.000000\n80.000000\n135.000000\n90.000000\n1.000000\n1.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nmax\n99999.000000\n23713.000000\n2.000000\n180.000000\n106.000000\n163.000000\n100.000000\n3.000000\n3.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n# transforming the column AGE(measured in days) for Years\ndf['years'] = (df['age'] / 365).round().astype('int')\ndf.drop(['age'], axis='columns', inplace=True)\ndf.drop(['id'], axis='columns', inplace=True)\n\n# age_bin in quinquenium 5 years spam\ndf['age_bin'] = pd.cut(df['years'], [0,20,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100], \n                              labels=['0-20', '20-30', '30-35', '35-40','40-45','45-50','50-55','55-60','60-65','65-70','70-75','75-80','80-85','85-90','90-95','95-100'])\n\n\n# Adding Body Mass Index\ndf['bmi'] = df['weight']/((df['height']/100)**2)\n\n\n# transforming the column bmi in Body Mass Index Classes (1 to 6)\nrating = []\nfor row in df['bmi']:\n    if row &lt; 18.5 :    rating.append(1) #UnderWeight\n    elif row &gt; 18.5 and row  &lt; 24.9:   rating.append(2)#NormalWeight\n    elif row &gt; 24.9 and row &lt; 29.9:  rating.append(3)#OverWeight\n    elif row &gt; 29.9 and row &lt; 34.9:  rating.append(4)#ClassObesity_1\n    elif row &gt; 34.9 and row &lt; 39.9:  rating.append(5)#ClassObesity_2\n    elif row &gt; 39.9 and row &lt; 49.9:  rating.append(6)#ClassObesity_3\n    elif row &gt; 49.9:  rating.append('Error')\n        \n    else:           rating.append('Not_Rated')\n\n\ndf['BMI_Class'] = rating\ndf[\"BMI_Class\"].value_counts(normalize=True)\n\nBMI_Class\n3    0.391773\n2    0.361943\n4    0.179209\n5    0.055801\n6    0.009162\n1    0.002112\nName: proportion, dtype: float64\n\n\nMean Arterial Pressure (MAP) = 2 Diastollic Blood Pressure + Sistolic Blood Pressure / 3\n\ndf['MAP'] = ((2* df['ap_lo']) + df['ap_hi']) / 3\n#Creating Classes for MAP\nmap_values = []\nfor row in df['MAP']:\n    if row &lt; 69.9:    map_values.append(1) #Low\n    elif row &gt; 70 and row  &lt; 79.9:   map_values.append(2)#Normal\n    elif row &gt; 79.9 and row &lt; 89.9:  map_values.append(3)#Normal\n    elif row &gt; 89.9 and row &lt; 99.9:  map_values.append(4)#Normal\n    elif row &gt; 99.9 and row &lt; 109.9:  map_values.append(5)#High\n    elif row &gt; 109.9 and row &lt; 119.9:  map_values.append(6)#Normal\n    elif row &gt; 119.9:  map_values.append(7)\n        \n    else:           map_values.append('Not_Rated')\n#inserting MAP_Class Column\ndf['MAP_Class'] = map_values\n\n\n# Reordering Columns\ndf= df[[\"gender\",\"height\",\"weight\",\"bmi\",\"ap_hi\",\"ap_lo\",\"MAP\",\"years\",\"age_bin\",\"BMI_Class\",\"MAP_Class\",\"cholesterol\",\"gluc\",\"smoke\",\"active\",\"cardio\"]]\ndf.head()\n\n\n\n\n\n\n\n\ngender\nheight\nweight\nbmi\nap_hi\nap_lo\nMAP\nyears\nage_bin\nBMI_Class\nMAP_Class\ncholesterol\ngluc\nsmoke\nactive\ncardio\n\n\n\n\n0\n2\n168\n62.0\n21.967120\n110\n80\n90.000000\n50\n45-50\n2\n4\n1\n1\n0\n1\n0\n\n\n1\n1\n156\n85.0\n34.927679\n140\n90\n106.666667\n55\n50-55\n5\n5\n3\n1\n0\n1\n1\n\n\n2\n1\n165\n64.0\n23.507805\n130\n70\n90.000000\n52\n50-55\n2\n4\n3\n1\n0\n0\n1\n\n\n3\n2\n169\n82.0\n28.710479\n150\n100\n116.666667\n48\n45-50\n3\n6\n1\n1\n0\n1\n1\n\n\n4\n1\n156\n56.0\n23.011177\n100\n60\n73.333333\n48\n45-50\n2\n2\n1\n1\n0\n0\n0\n\n\n\n\n\n\n\n\nData Analize and Preparation\n\ndf_cat = df[[\"gender\",\"age_bin\",\"BMI_Class\",\"MAP_Class\",\"cholesterol\",\"gluc\",\"smoke\",\"active\",\"cardio\",]]\n\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf_cat = df_cat.apply(le.fit_transform)\ndf_cat.head()\n\n\n\n\n\n\n\n\ngender\nage_bin\nBMI_Class\nMAP_Class\ncholesterol\ngluc\nsmoke\nactive\ncardio\n\n\n\n\n0\n1\n3\n1\n2\n0\n0\n0\n1\n0\n\n\n1\n0\n4\n4\n3\n2\n0\n0\n1\n1\n\n\n2\n0\n4\n1\n2\n2\n0\n0\n0\n1\n\n\n3\n1\n3\n2\n4\n0\n0\n0\n1\n1\n\n\n4\n0\n3\n1\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\ndf_male = df_cat.query(\"gender == 0\")\ndf_female = df_cat.query(\"gender == 1\")\ndf_male.head()\n\n\n\n\n\n\n\n\ngender\nage_bin\nBMI_Class\nMAP_Class\ncholesterol\ngluc\nsmoke\nactive\ncardio\n\n\n\n\n1\n0\n4\n4\n3\n2\n0\n0\n1\n1\n\n\n2\n0\n4\n1\n2\n2\n0\n0\n0\n1\n\n\n4\n0\n3\n1\n0\n0\n0\n0\n0\n0\n\n\n5\n0\n5\n2\n2\n1\n1\n0\n0\n0\n\n\n6\n0\n6\n4\n2\n2\n0\n0\n1\n0\n\n\n\n\n\n\n\n\nf, axs = plt.subplots(1,3,figsize = (12,5))\nsns.countplot(x=df_cat['MAP_Class'],order=df_cat['MAP_Class'].value_counts().index,hue=df_cat['cardio'],ax=axs[0])\nsns.countplot(x=df_male['MAP_Class'],order=df_male['MAP_Class'].value_counts().index,hue=df_male['cardio'],ax=axs[1])\nsns.countplot(x=df_female['MAP_Class'],order=df_female['MAP_Class'].value_counts().index,hue=df_female['cardio'],ax=axs[2])\n\naxs[0].set_title('All Data')\naxs[1].set_title('Male Data')\naxs[2].set_title('Female Data')\nplt.tight_layout()\nplt.show()\n\n\n\n\nAll the values are similar\n\nf, axs = plt.subplots(1,3,figsize = (12,5))\nsns.countplot(x=df_cat['BMI_Class'],order=df_cat['BMI_Class'].value_counts().index,hue=df_cat['cardio'],ax=axs[0])\nsns.countplot(x=df_male['BMI_Class'],order=df_male['BMI_Class'].value_counts().index,hue=df_male['cardio'],ax=axs[1])\nsns.countplot(x=df_female['BMI_Class'],order=df_female['BMI_Class'].value_counts().index,hue=df_female['cardio'],ax=axs[2])\n\naxs[0].set_title('All Data')\naxs[1].set_title('Male Data')\naxs[2].set_title('Female Data')\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe BMI classes across all data, as well as for males and females, are quite similar. However, there’s a slight variance in the BMI_Class = 2 specifically for the female dataset.\n\nf, axs = plt.subplots(1,3,figsize = (12,5))\nsns.countplot(x=df_cat['gluc'],order=df_cat['gluc'].value_counts().index,hue=df_cat['cardio'],ax=axs[0])\nsns.countplot(x=df_male['gluc'],order=df_male['gluc'].value_counts().index,hue=df_male['cardio'],ax=axs[1])\nsns.countplot(x=df_female['gluc'],order=df_female['gluc'].value_counts().index,hue=df_female['cardio'],ax=axs[2])\n\naxs[0].set_title('All Data')\naxs[1].set_title('Male Data')\naxs[2].set_title('Female Data')\nplt.tight_layout()\nplt.show()\n\n\n\n\nAll the values are similar\n\nf, axs = plt.subplots(1,3,figsize = (12,5))\nsns.countplot(x=df_cat['cholesterol'],order=df_cat['cholesterol'].value_counts().index,hue=df_cat['cardio'],ax=axs[0])\nsns.countplot(x=df_male['cholesterol'],order=df_male['cholesterol'].value_counts().index,hue=df_male['cardio'],ax=axs[1])\nsns.countplot(x=df_female['cholesterol'],order=df_female['cholesterol'].value_counts().index,hue=df_female['cardio'],ax=axs[2])\n\naxs[0].set_title('All Data')\naxs[1].set_title('Male Data')\naxs[2].set_title('Female Data')\nplt.tight_layout()\nplt.show()\n\n\n\n\nAll the values are similar\n\n\nKModes Clustering Analysis\n\n# new df to work\ndf_male = df_cat.query(\"gender == 0\")\ndf_female = df_cat.query(\"gender == 1\")\n\n\n# Elbow curve to find optimal K in Huang init\ncost = []\nK = range(1,6)\nfor num_clusters in list(K):\n    kmode = KModes(n_clusters=num_clusters, init = \"Huang\", n_init = 5, verbose=0)\n    kmode.fit_predict(df_cat)\n    cost.append(kmode.cost_)\n    \nplt.plot(K, cost, 'bx-')\nplt.xlabel('No. of clusters')\nplt.ylabel('Cost')\nplt.title('Elbow Method For Optimal k')\nplt.show()\n\n\n\n\n\n# female data\n# Building the model with using K-Mode with \"Huang\" initialization\nkm_huang = KModes(n_clusters=2, init = \"Huang\", n_init = 5, verbose=0)\nclusters_huang_1 = km_huang.fit_predict(df_female)\nclusters_huang_1\n\narray([1, 0, 0, ..., 0, 1, 1], dtype=uint16)\n\n\n\n# male data\n# Building the model with using K-Mode with \"Huang\" initialization\nkm_huang = KModes(n_clusters=2, init = \"Huang\", n_init = 5, verbose=0)\nclusters_huang_2 = km_huang.fit_predict(df_male)\nclusters_huang_2\n\narray([0, 1, 1, ..., 0, 0, 0], dtype=uint16)\n\n\n\ndf_female.insert(0,\"Cluster\", clusters_huang_1, True)\n\n# female DataFrame with Clusters\ndf_female.head()\n\n\n\n\n\n\n\n\nCluster\ngender\nage_bin\nBMI_Class\nMAP_Class\ncholesterol\ngluc\nsmoke\nactive\ncardio\n\n\n\n\n0\n1\n1\n3\n1\n2\n0\n0\n0\n1\n0\n\n\n3\n0\n1\n3\n2\n4\n0\n0\n0\n1\n1\n\n\n7\n0\n1\n6\n3\n3\n2\n2\n0\n1\n1\n\n\n11\n1\n1\n4\n1\n2\n0\n0\n0\n1\n0\n\n\n12\n1\n1\n2\n1\n2\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\ndf_male.insert(0, \"Cluster\", clusters_huang_2, True)\n\n# female DataFrame with Clusters\ndf_male.head()\n\n\n\n\n\n\n\n\nCluster\ngender\nage_bin\nBMI_Class\nMAP_Class\ncholesterol\ngluc\nsmoke\nactive\ncardio\n\n\n\n\n1\n0\n0\n4\n4\n3\n2\n0\n0\n1\n1\n\n\n2\n1\n0\n4\n1\n2\n2\n0\n0\n0\n1\n\n\n4\n1\n0\n3\n1\n0\n0\n0\n0\n0\n0\n\n\n5\n0\n0\n5\n2\n2\n1\n1\n0\n0\n0\n\n\n6\n1\n0\n6\n4\n2\n2\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n# replacing cluster column values to merge dataframes after\ndf_male[\"Cluster\"].replace({0:2, 1:3}, inplace=True)\ndf_male.head()\n\n/tmp/ipykernel_23416/1028254110.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_male[\"Cluster\"].replace({0:2, 1:3}, inplace=True)\n\n\n\n\n\n\n\n\n\nCluster\ngender\nage_bin\nBMI_Class\nMAP_Class\ncholesterol\ngluc\nsmoke\nactive\ncardio\n\n\n\n\n1\n2\n0\n4\n4\n3\n2\n0\n0\n1\n1\n\n\n2\n3\n0\n4\n1\n2\n2\n0\n0\n0\n1\n\n\n4\n3\n0\n3\n1\n0\n0\n0\n0\n0\n0\n\n\n5\n2\n0\n5\n2\n2\n1\n1\n0\n0\n0\n\n\n6\n3\n0\n6\n4\n2\n2\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n# female\nf, axs = plt.subplots(1,3,figsize = (12,5))\nsns.countplot(x=df_female['Cluster'],order=df_female['Cluster'].value_counts().index,hue=df_female['cardio'],ax=axs[0],palette='rainbow')\nsns.countplot(x=df_female['smoke'],order=df_female['smoke'].value_counts().index,hue=df_female['cardio'],ax=axs[1],palette='rainbow')\nsns.countplot(x=df_female['BMI_Class'],order=df_female['BMI_Class'].value_counts().index,hue=df_female['cardio'],ax=axs[2],palette='rainbow')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# male\nf, axs = plt.subplots(1,3,figsize = (12,5))\nsns.countplot(x=df_male['Cluster'],order=df_male['Cluster'].value_counts().index,hue=df_male['cardio'],ax=axs[0],palette='rainbow')\nsns.countplot(x=df_male['smoke'],order=df_male['smoke'].value_counts().index,hue=df_male['cardio'],ax=axs[1],palette='rainbow')\nsns.countplot(x=df_male['BMI_Class'],order=df_male['BMI_Class'].value_counts().index,hue=df_male['cardio'],ax=axs[2],palette='rainbow')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html",
    "href": "posts/nlinregression/index.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Contents:"
  },
  {
    "objectID": "posts/nlinregression/index.html#example-of-nonlinear-regression",
    "href": "posts/nlinregression/index.html#example-of-nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Example of Nonlinear Regression",
    "text": "Example of Nonlinear Regression\nI.Introduction\nIf the data shows a curvy trend, then linear regression will not produce very accurate results when compared to a non-linear regression because, as the name implies, linear regression presumes that the data is linear.\nImporting required libraries\n\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization"
  },
  {
    "objectID": "posts/nlinregression/index.html#linear",
    "href": "posts/nlinregression/index.html#linear",
    "title": "Nonlinear Regression",
    "section": "1. Linear",
    "text": "1. Linear\n\nx = np.arange(-6.0, 6.0, 0.1)\ny = 3*(x) + 2\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#polynomial",
    "href": "posts/nlinregression/index.html#polynomial",
    "title": "Nonlinear Regression",
    "section": "2. Polynomial",
    "text": "2. Polynomial\n\nx = np.arange(-6.0, 6.0, 0.1)\ny = 1*(x**3) + 2*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#quadratic",
    "href": "posts/nlinregression/index.html#quadratic",
    "title": "Nonlinear Regression",
    "section": "3. Quadratic",
    "text": "3. Quadratic\n\nx = np.arange(-6.0, 6.0, 0.1)\n\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#exponential",
    "href": "posts/nlinregression/index.html#exponential",
    "title": "Nonlinear Regression",
    "section": "4. Exponential",
    "text": "4. Exponential\n\nX = np.arange(-6.0, 6.0, 0.1)\n\nY= np.exp(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#logarithmic",
    "href": "posts/nlinregression/index.html#logarithmic",
    "title": "Nonlinear Regression",
    "section": "5. Logarithmic",
    "text": "5. Logarithmic\n\nX = np.arange(1.0, 10.0, 0.1)\n\nY = np.log(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#sigmoidallogistic",
    "href": "posts/nlinregression/index.html#sigmoidallogistic",
    "title": "Nonlinear Regression",
    "section": "6. Sigmoidal/Logistic",
    "text": "6. Sigmoidal/Logistic\n\nX = np.arange(-5.0, 5.0, 0.1)\n\n\nY = 1-4/(1+np.power(3, X-2))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#non-linear-regression-example-with-dataset",
    "href": "posts/nlinregression/index.html#non-linear-regression-example-with-dataset",
    "title": "Nonlinear Regression",
    "section": "Non-Linear Regression example with Dataset",
    "text": "Non-Linear Regression example with Dataset\n\ndf1 = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/nlinregression/gdp.csv')\ndf1.head()\ndf = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/nlinregression/gdp1.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n\n\nNumber of rows and columns in the data set:  (55, 2)\n\n\n\n\n\n\n\n\n\n\nYear\nValue\n\n\n\n\n0\n1960\n5.918412e+10\n\n\n1\n1961\n4.955705e+10\n\n\n2\n1962\n4.668518e+10\n\n\n3\n1963\n5.009730e+10\n\n\n4\n1964\n5.906225e+10\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,5))\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#section",
    "href": "posts/nlinregression/index.html#section",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Choosing a model\nFrom an initial look at the plot, we determine that the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\n\nX = np.arange(-5,5.0, 0.1)\nY = 1.0 / (1.0 + np.exp(-X))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\nBuilding The Model\nNow, let’s build our regression model and initialize its parameters.\n\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n   \n   \nbeta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\n\n\n\n\n\n# Lets normalize our data\nxdata =x_data/max(x_data)\nydata =y_data/max(y_data)\n\n\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 690.451709, beta_2 = 0.997207\n\n\n\nx = np.linspace(1960, 2015, 55)\nx = x/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\n\n# split data into train/test\nmsk = np.random.rand(len(df)) &lt; 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )\n\nMean absolute error: 0.04\nResidual sum of squares (MSE): 0.00\nR2-score: 0.97"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "MLBlog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probability and Random Variables",
    "section": "",
    "text": "Contents:"
  },
  {
    "objectID": "posts/probability/index.html#random-variable",
    "href": "posts/probability/index.html#random-variable",
    "title": "Probability and Random Variables",
    "section": "Random Variable",
    "text": "Random Variable\nA random variable serves as a mathematical link between various numerical values and the outcomes of an experiment. It assigns distinct values to different results of the experiment, making it variable. As each outcome holds an element of uncertainty, the variable is considered random.\nIn probability, a common representation for a random variable is using a capital letter (e.g., X), and a specific value is denoted by a corresponding lowercase letter (e.g., x). For instance, when flipping a coin twice, the potential outcomes form the sample space:\nS = {HH, HT, TH, TT}\nFrom this sample space, the random variable X can be defined as:\nX = {HH, HT, TH, TT}\nRandom variables are categorized into two types: continuous and discrete random variables.\nDiscrete random variables are limited to specific, whole-number values and do not include fractions or decimals. They generate discrete probability distributions.\nExamples include the total outcome of rolling two dice (ranging from 2 to 12) or the count of desktops sold (starting from 0 and increasing by whole numbers).\nIn application, discrete random variables describe distinct quantities, such as the number of aircraft taking off and landing at an airport at a given time or the specific count of communication lines activated in an organization’s system.\nOn the other hand, continuous random variables encompass a broad range of values, including decimals and fractions. They give rise to continuous probability distributions.\nExamples of continuous random variables involve quantities like interest rates (such as 4.55% or 7.9%) or durations like task completion times.\nIn practical scenarios, continuous random variables are employed to represent measurements like reaction temperature errors or the probability of a construction project completing within a specific timeframe, such as between 20 and 24 months with a probability of 0.5."
  },
  {
    "objectID": "posts/probability/index.html#probability",
    "href": "posts/probability/index.html#probability",
    "title": "Probability and Random Variables",
    "section": "Probability",
    "text": "Probability\nProbability quantifies the likelihood of an event occurring, expressed as percentages or descriptive terms like “impossible” or “probable.” It encompasses the chances of various outcomes, such as rolling a specific number on a die. For instance, the probability of rolling a “4” on a six-sided die is 1 out of 6, considering there’s only one favorable outcome among six possibilities."
  },
  {
    "objectID": "posts/probability/index.html#why-probability",
    "href": "posts/probability/index.html#why-probability",
    "title": "Probability and Random Variables",
    "section": "Why Probability?",
    "text": "Why Probability?\nIn the realm of machine learning, uncertainty and stochastic elements often emerge due to incomplete observability, leading us to work primarily with sampled data.\nConsider a scenario where we aim to make reliable inferences about the behavior of a random variable, despite having access only to limited data, leaving the entire population characteristics unknown.\n\n\n\nEstimating the data-generating process (src: https://towardsdatascience.com/understanding-random-variables-and-probability-distributions-1ed1daf2e66)\n\n\nTherefore, we require a method to extrapolate from the sampled data to represent the entire population or, in simpler terms, estimate the true process generating the data. Understanding the probability distribution becomes crucial as it allows us to gauge the likelihood of specific outcomes while accommodating the variability observed in the results. This comprehension enables us to extend conclusions from the sample to the broader population, approximate the function generating the data, and enhance the accuracy of predicting the behavior of a random variable.\n\nDiscrete Probability Distribution\nDiscrete probability distributions stem from discrete data and aim to model predictions or outcomes, such as pricing options or forecasting market shocks. These distributions illustrate the possible values of a discrete random variable along with their corresponding probabilities. Examples include the Bernoulli, geometric, and binomial distributions. Here are some common examples of discrete probability distributions:\n\nBernoulli Distribution: Models a single trial with two outcomes, often used in success/failure experiments.\nBinomial Distribution: Describes the number of successes in a fixed number of independent trials with a constant probability of success.\nPoisson Distribution: Models the number of events occurring in a fixed interval of time or space, given a known average rate of occurrence.\n\n\n\nContinuous Probability Distribution\nIn contrast, continuous probability distributions cover an infinite range of values, making them uncountable, like time extending from 0 seconds indefinitely. Examples involve continuous measurements like annual rainfall in a city or the weight of newborn babies, where the range of values is limitless and not countable in a finite manner. Here are some common examples of continuous probability distributions:\n\nNormal (Gaussian) Distribution: Symmetric bell-shaped curve that describes many natural phenomena like heights or test scores.\nUniform Distribution: All values in an interval have equal probability, forming a rectangle in the probability density function.\nExponential Distribution: Models the time between events in a Poisson process, such as the time between phone calls at a call center.\n\n\nExample of discrete probability distribution\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the Titanic dataset\ntitanic_data = sns.load_dataset('titanic')\n\n# Filter data for relevant columns (class and survival)\nsurvival_data = titanic_data[['pclass', 'survived']]\n\n# Calculate survival probabilities based on passenger class\nsurvival_probabilities = survival_data.groupby('pclass')['survived'].mean()\n\n# Plotting the probability distribution\nplt.figure(figsize=(8, 6))\nsurvival_probabilities.plot(kind='bar', color='skyblue')\nplt.title('Survival Probability by Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Survival Probability')\nplt.xticks(rotation=0)\nplt.ylim(0, 1)  # Setting y-axis limits to probability range (0 to 1)\nplt.show()\n\n\n\n\n\n\nExample of continuous probability distribution\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom scipy.stats import norm, expon\n\n# Load breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Consider a specific feature for demonstration (e.g., mean radius)\nfeature_index = 0\nselected_feature = X[:, feature_index]\n\n# Summary statistics and visualization\nmean_value = np.mean(selected_feature)\nstd_dev = np.std(selected_feature)\n\nprint(f\"Mean: {mean_value}, Standard Deviation: {std_dev}\")\n\n# Plotting the histogram of the selected feature\nplt.figure(figsize=(8, 6))\nplt.hist(selected_feature, bins=30, density=True, alpha=0.6, color='blue')\n\n# Plot Gaussian distribution based on the observed mean and standard deviation\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean_value, std_dev)\nplt.plot(x, p, 'k', linewidth=2, label='Gaussian PDF')\nplt.title('Histogram and Gaussian PDF for Mean Radius')\nplt.xlabel('Mean Radius')\nplt.ylabel('Frequency/Probability')\nplt.legend()\nplt.show()\n\n# Plotting an exponential distribution\nplt.figure(figsize=(8, 6))\n\n# Generate random data following an exponential distribution with the observed mean\nexponential_data = np.random.exponential(scale=mean_value, size=1000)\n\n# Plot histogram\nplt.hist(exponential_data, bins=30, density=True, alpha=0.6, color='green')\n\n# Plot exponential distribution PDF\nx_exp = np.linspace(0, np.max(exponential_data), 100)\np_exp = expon.pdf(x_exp, scale=mean_value)\nplt.plot(x_exp, p_exp, 'k', linewidth=2, label='Exponential PDF')\nplt.title('Histogram and Exponential PDF for Mean Radius')\nplt.xlabel('Mean Radius')\nplt.ylabel('Frequency/Probability')\nplt.legend()\nplt.show()\n\nMean: 14.127291739894552, Standard Deviation: 3.520950760711062\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\n\n# Select a few features for analysis (e.g., mean radius, mean texture, mean perimeter)\nselected_features = ['mean radius', 'mean texture', 'mean perimeter']\n\n# Descriptive statistics\nprint(df[selected_features].describe())\n\n# Visualize the data using pair plots\nsns.pairplot(df[selected_features])\nplt.show()\n\n# Create probability distribution plots for selected features\nplt.figure(figsize=(15, 5))\nfor i, feature in enumerate(selected_features, 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(df[feature], kde=True, stat=\"probability\", bins=20)\n    plt.title(f'Probability Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Probability')\nplt.tight_layout()\nplt.show()\n\n       mean radius  mean texture  mean perimeter\ncount   569.000000    569.000000      569.000000\nmean     14.127292     19.289649       91.969033\nstd       3.524049      4.301036       24.298981\nmin       6.981000      9.710000       43.790000\n25%      11.700000     16.170000       75.170000\n50%      13.370000     18.840000       86.240000\n75%      15.780000     21.800000      104.100000\nmax      28.110000     39.280000      188.500000"
  }
]