print('~'*50)
print("sum of elements that contain in cluster 0 :",(labels == 0).sum())
print("sum of elements that contain in cluster 1 :",(labels == 1).sum())
print("sum of elements that contain in cluster 2 :",(labels == 2).sum())
print("sum of elements that contain in cluster 3 :",(labels == 3).sum())
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.metrics import silhouette_score
score = silhouette_score(df_transformed,  labels, metric='euclidean')
print('Silhouett Score: %.3f' % score)
visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')
visualizer.fit(df_transformed)
visualizer.show()
from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components=10, kernel='rbf')
df_kpca = pd.DataFrame(kpca.fit_transform(df_transformed))
df_kpca
from sklearn.ensemble import IsolationForest
clf = IsolationForest(random_state=0,
max_features=2,
n_estimators=100,
contamination=0.1).fit(df_kpca)
anom_pred = clf.predict(df_kpca)
anom_pred
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2,
perplexity=50,
random_state=42,
n_iter=300).fit_transform(df_transformed)
df_embed_Iso = pd.DataFrame(tsne, columns=['feature1', 'feature2'])
df_embed_Iso['Labels']= pd.DataFrame(anom_pred)
df_embed_Iso
plt.figure(figsize=(12,8))
sns.scatterplot(
x='feature1', y='feature2',
data=df_embed_Iso,
hue=df_embed_Iso['Labels'],
palette=sns.color_palette("hls", 2)
)
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import matplotlib.font_manager
from pyod.models.knn import KNN
from pyod.utils.data import generate_data, get_outliers_inliers
# generating a random dataset with two features
X_train, y_train = generate_data(n_train = 300, train_only = True,
n_features = 2)
# Setting the percentage of outliers
outlier_fraction = 0.1
# Storing the outliers and inliners in different numpy arrays
X_outliers, X_inliers = get_outliers_inliers(X_train, y_train)
n_inliers = len(X_inliers)
n_outliers = len(X_outliers)
# Separating the two features
f1 = X_train[:, [0]].reshape(-1, 1)
f2 = X_train[:, [1]].reshape(-1, 1)
xx, yy = np.meshgrid(np.linspace(-10, 10, 200),
np.linspace(-10, 10, 200))
# scatter plot
plt.scatter(f1, f2)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
clf = KNN(contamination = outlier_fraction)
clf.fit(X_train, y_train)
# You can print this to see all the prediction scores
scores_pred = clf.decision_function(X_train)*-1
y_pred = clf.predict(X_train)
n_errors = (y_pred != y_train).sum()
# Counting the number of errors
print('The number of prediction errors are ' + str(n_errors))
threshold = stats.scoreatpercentile(scores_pred, 100 * outlier_fraction)
# decision function calculates the raw
# anomaly score for every point
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1
Z = Z.reshape(xx.shape)
# fill blue colormap from minimum anomaly
# score to threshold value
subplot = plt.subplot(1, 2, 1)
subplot.contourf(xx, yy, Z, levels = np.linspace(Z.min(),
threshold, 10), cmap = plt.cm.Blues_r)
# draw red contour line where anomaly
# score is equal to threshold
a = subplot.contour(xx, yy, Z, levels =[threshold],
linewidths = 2, colors ='red')
# fill orange contour lines where range of anomaly
# score is from threshold to maximum anomaly score
subplot.contourf(xx, yy, Z, levels =[threshold, Z.max()], colors ='orange')
# scatter plot of inliers with white dots
b = subplot.scatter(X_train[:-n_outliers, 0], X_train[:-n_outliers, 1],
c ='white', s = 20, edgecolor ='k')
# scatter plot of outliers with black dots
c = subplot.scatter(X_train[-n_outliers:, 0], X_train[-n_outliers:, 1],
c ='black', s = 20, edgecolor ='k')
subplot.axis('tight')
subplot.legend(
[a.collections[0], b, c],
['learned decision function', 'true inliers', 'true outliers'],
prop = matplotlib.font_manager.FontProperties(size = 10),
loc ='lower right')
subplot.set_title('K-Nearest Neighbours')
subplot.set_xlim((-10, 10))
subplot.set_ylim((-10, 10))
plt.show()
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.ensemble import IsolationForest
from sklearn.metrics import silhouette_score
warnings.filterwarnings("ignore")
df = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/outlier-detection/CC GENERAL.csv')
print('The shape of the dataset is:', df.shape)
# check number of nulls in each column
df.isnull().sum().sort_values(ascending=False)
# konw the ratio of null in each column
round(df.isnull().sum(axis=0)*100/df.shape[0],2).sort_values(ascending=False)
# save numeric columns and objects in separeted list to handle each one of them
numeric_columns = df.select_dtypes(exclude=['object']).columns.to_list()
object_columns = df.select_dtypes(include=['object']).columns.to_list()
df[numeric_columns].hist(bins=15, figsize=(20,15))
plt.subplots(figsize=(15, 15))
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)
for i, column in enumerate(numeric_columns, 1):
plt.subplot(7, 3, i)
sns.boxplot(df[column])
df_pre=df.copy()
df_pre.drop(object_columns, axis=1, inplace=True)
df_pre.head(3)
columns_names = list(df_pre.columns)
columns_names
from sklearn.impute import SimpleImputer
df_NoNull = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(df_pre), columns=columns_names)
df_NoNull
# will add 1 to all values because log transform get error for numbers between 0 and 1
df_pre2 = (df_NoNull + 1)
df_log = np.log(df_pre2)
df_log.describe().T
f, axs = plt.subplots(figsize=(15, 15))
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)
for i, column in enumerate(df_log.columns, 1):
plt.subplot(7, 3, i)
sns.boxplot(df_log[column])
df_pre2 = df_NoNull.copy()
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import PowerTransformer
df_power = PowerTransformer(method="yeo-johnson").fit_transform(df_pre2)
df_power= pd.DataFrame(df_power, columns=columns_names)
df_power.describe().T
f, axs = plt.subplots(figsize=(15, 15))
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)
for i, column in enumerate(df_power.columns, 1):
plt.subplot(7, 3, i)
sns.boxplot(df_power[column])
df_power.hist(bins=20, figsize=(20,15))
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
scale_MinMax = MinMaxScaler()
df_transformed = pd.DataFrame(scale_MinMax.fit_transform(df_NoNull), columns=columns_names)
scale_MinMax = MinMaxScaler()
df_transformed_Log = pd.DataFrame(scale_MinMax.fit_transform(df_log), columns=columns_names)
scale_MinMax = MinMaxScaler()
df_transformed_Power = pd.DataFrame(scale_MinMax.fit_transform(df_power), columns=columns_names)
scale_Standard = StandardScaler()
df_transformed_Power = pd.DataFrame(scale_Standard.fit_transform(df_power), columns=columns_names)
df_log.to_csv("./Data_Log.csv",index=False)
df_transformed = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/outlier-detection/Data_Log.csv')
df_transformed
# To plot Elbow With Inertia
inertia = []
Range = [*range(1,11)]
for k in Range:
kmean = KMeans(n_clusters=k, max_iter=300, random_state=42)
kmean.fit(df_transformed)
inertia.append(kmean.inertia_)
plt.figure(figsize=(10,4))
plt.plot(Range, inertia, 'bx-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('The Elbow Method ')
plt.show()
drop_variation = []
drop_variation.append(0) #add 0 in the first element
for i in range(len(inertia) -1):
dropValue = inertia[i] - inertia[i+1]
drop_variation.append(dropValue)
# select suitable k that have large drop in the variation
k = Range[np.argmax(drop_variation)]
print("Suitable number of clusters = ",k)
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, random_state=42).fit(df_transformed)
labels = kmeans.labels_
iner = kmeans.inertia_
cent = kmeans.cluster_centers_
print("\t~~ THIS RESULT OF K-mean SKLEARN  ~~")
print('~'*50)
print("sum of elements that contain in cluster 0 :",(labels == 0).sum())
print("sum of elements that contain in cluster 1 :",(labels == 1).sum())
print("sum of elements that contain in cluster 2 :",(labels == 2).sum())
print("sum of elements that contain in cluster 3 :",(labels == 3).sum())
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.metrics import silhouette_score
score = silhouette_score(df_transformed,  labels, metric='euclidean')
print('Silhouett Score: %.3f' % score)
visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')
visualizer.fit(df_transformed)
visualizer.show()
from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components=10, kernel='rbf')
df_kpca = pd.DataFrame(kpca.fit_transform(df_transformed))
df_kpca
from sklearn.ensemble import IsolationForest
clf = IsolationForest(random_state=0,
max_features=2,
n_estimators=100,
contamination=0.1).fit(df_kpca)
anom_pred = clf.predict(df_kpca)
anom_pred
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2,
perplexity=50,
random_state=42,
n_iter=300).fit_transform(df_transformed)
df_embed_Iso = pd.DataFrame(tsne, columns=['feature1', 'feature2'])
df_embed_Iso['Labels']= pd.DataFrame(anom_pred)
df_embed_Iso
plt.figure(figsize=(12,8))
sns.scatterplot(
x='feature1', y='feature2',
data=df_embed_Iso,
hue=df_embed_Iso['Labels'],
palette=sns.color_palette("hls", 2)
)
# generating a random dataset with two features
X_train, y_train = generate_data(n_train = 300, train_only = True,
n_features = 2)
# Setting the percentage of outliers
outlier_fraction = 0.1
# Storing the outliers and inliners in different numpy arrays
X_outliers, X_inliers = get_outliers_inliers(X_train, y_train)
n_inliers = len(X_inliers)
n_outliers = len(X_outliers)
# Separating the two features
f1 = X_train[:, [0]].reshape(-1, 1)
f2 = X_train[:, [1]].reshape(-1, 1)
xx, yy = np.meshgrid(np.linspace(-10, 10, 200),
np.linspace(-10, 10, 200))
# scatter plot
plt.scatter(f1, f2)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
clf = KNN(contamination = outlier_fraction)
clf.fit(X_train, y_train)
# You can print this to see all the prediction scores
scores_pred = clf.decision_function(X_train)*-1
y_pred = clf.predict(X_train)
n_errors = (y_pred != y_train).sum()
# Counting the number of errors
print('The number of prediction errors are ' + str(n_errors))
clf = KNN(contamination = outlier_fraction)
clf.fit(X_train, y_train)
# You can print this to see all the prediction scores
scores_pred = clf.decision_function(X_train)*-1
y_pred = clf.predict(X_train)
n_errors = (y_pred != y_train).sum()
# Counting the number of errors
print('The number of prediction errors are ' + str(n_errors))
threshold = stats.scoreatpercentile(scores_pred, 100 * outlier_fraction)
# decision function calculates the raw
# anomaly score for every point
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1
Z = Z.reshape(xx.shape)
# fill blue colormap from minimum anomaly
# score to threshold value
subplot = plt.subplot(1, 2, 1)
subplot.contourf(xx, yy, Z, levels = np.linspace(Z.min(),
threshold, 10), cmap = plt.cm.Blues_r)
# draw red contour line where anomaly
# score is equal to threshold
a = subplot.contour(xx, yy, Z, levels =[threshold],
linewidths = 2, colors ='red')
# fill orange contour lines where range of anomaly
# score is from threshold to maximum anomaly score
subplot.contourf(xx, yy, Z, levels =[threshold, Z.max()], colors ='orange')
# scatter plot of inliers with white dots
b = subplot.scatter(X_train[:-n_outliers, 0], X_train[:-n_outliers, 1],
c ='white', s = 20, edgecolor ='k')
# scatter plot of outliers with black dots
c = subplot.scatter(X_train[-n_outliers:, 0], X_train[-n_outliers:, 1],
c ='black', s = 20, edgecolor ='k')
subplot.axis('tight')
subplot.legend(
[a.collections[0], b, c],
['learned decision function', 'true inliers', 'true outliers'],
prop = matplotlib.font_manager.FontProperties(size = 10),
loc ='lower right')
subplot.set_title('K-Nearest Neighbours')
subplot.set_xlim((-10, 10))
subplot.set_ylim((-10, 10))
plt.show()
plt.subplots(figsize=(15, 15))
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)
for i, column in enumerate(numeric_columns, 1):
plt.subplot(7, 3, i)
sns.boxplot(df[column], orient='h')
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import matplotlib.font_manager
from pyod.models.knn import KNN
from pyod.utils.data import generate_data, get_outliers_inliers
# generating a random dataset with two features
X_train, y_train = generate_data(n_train = 300, train_only = True,
n_features = 2)
# Setting the percentage of outliers
outlier_fraction = 0.1
# Storing the outliers and inliners in different numpy arrays
X_outliers, X_inliers = get_outliers_inliers(X_train, y_train)
n_inliers = len(X_inliers)
n_outliers = len(X_outliers)
# Separating the two features
f1 = X_train[:, [0]].reshape(-1, 1)
f2 = X_train[:, [1]].reshape(-1, 1)
xx, yy = np.meshgrid(np.linspace(-10, 10, 200),
np.linspace(-10, 10, 200))
# scatter plot
plt.scatter(f1, f2)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
clf = KNN(contamination = outlier_fraction)
clf.fit(X_train, y_train)
# You can print this to see all the prediction scores
scores_pred = clf.decision_function(X_train)*-1
y_pred = clf.predict(X_train)
n_errors = (y_pred != y_train).sum()
# Counting the number of errors
print('The number of prediction errors are ' + str(n_errors))
threshold = stats.scoreatpercentile(scores_pred, 100 * outlier_fraction)
# decision function calculates the raw
# anomaly score for every point
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1
Z = Z.reshape(xx.shape)
# fill blue colormap from minimum anomaly
# score to threshold value
subplot = plt.subplot(1, 2, 1)
subplot.contourf(xx, yy, Z, levels = np.linspace(Z.min(),
threshold, 10), cmap = plt.cm.Blues_r)
# draw red contour line where anomaly
# score is equal to threshold
a = subplot.contour(xx, yy, Z, levels =[threshold],
linewidths = 2, colors ='red')
# fill orange contour lines where range of anomaly
# score is from threshold to maximum anomaly score
subplot.contourf(xx, yy, Z, levels =[threshold, Z.max()], colors ='orange')
# scatter plot of inliers with white dots
b = subplot.scatter(X_train[:-n_outliers, 0], X_train[:-n_outliers, 1],
c ='white', s = 20, edgecolor ='k')
# scatter plot of outliers with black dots
c = subplot.scatter(X_train[-n_outliers:, 0], X_train[-n_outliers:, 1],
c ='black', s = 20, edgecolor ='k')
subplot.axis('tight')
subplot.legend(
[a.collections[0], b, c],
['learned decision function', 'true inliers', 'true outliers'],
prop = matplotlib.font_manager.FontProperties(size = 10),
loc ='lower right')
subplot.set_title('K-Nearest Neighbours')
subplot.set_xlim((-10, 10))
subplot.set_ylim((-10, 10))
plt.show()
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.ensemble import IsolationForest
from sklearn.metrics import silhouette_score
warnings.filterwarnings("ignore")
df = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/outlier-detection/CC GENERAL.csv')
print('The shape of the dataset is:', df.shape)
# check number of nulls in each column
df.isnull().sum().sort_values(ascending=False)
# konw the ratio of null in each column
round(df.isnull().sum(axis=0)*100/df.shape[0],2).sort_values(ascending=False)
# save numeric columns and objects in separeted list to handle each one of them
numeric_columns = df.select_dtypes(exclude=['object']).columns.to_list()
object_columns = df.select_dtypes(include=['object']).columns.to_list()
df[numeric_columns].hist(bins=15, figsize=(20,15))
plt.subplots(figsize=(15, 15))
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)
for i, column in enumerate(numeric_columns, 1):
plt.subplot(7, 3, i)
sns.boxplot(df[column], orient='h')
df_pre=df.copy()
df_pre.drop(object_columns, axis=1, inplace=True)
df_pre.head(3)
columns_names = list(df_pre.columns)
columns_names
from sklearn.impute import SimpleImputer
df_NoNull = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(df_pre), columns=columns_names)
df_NoNull
# will add 1 to all values because log transform get error for numbers between 0 and 1
df_pre2 = (df_NoNull + 1)
df_log = np.log(df_pre2)
df_log.describe().T
f, axs = plt.subplots(figsize=(15, 15))
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)
for i, column in enumerate(df_log.columns, 1):
plt.subplot(7, 3, i)
sns.boxplot(df_log[column], orient='h')
df_pre2 = df_NoNull.copy()
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import PowerTransformer
df_power = PowerTransformer(method="yeo-johnson").fit_transform(df_pre2)
df_power= pd.DataFrame(df_power, columns=columns_names)
df_power.describe().T
f, axs = plt.subplots(figsize=(15, 15))
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)
for i, column in enumerate(df_power.columns, 1):
plt.subplot(7, 3, i)
sns.boxplot(df_power[column], orient='h')
df_power.hist(bins=20, figsize=(20,15))
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
scale_MinMax = MinMaxScaler()
df_transformed = pd.DataFrame(scale_MinMax.fit_transform(df_NoNull), columns=columns_names)
scale_MinMax = MinMaxScaler()
df_transformed_Log = pd.DataFrame(scale_MinMax.fit_transform(df_log), columns=columns_names)
scale_MinMax = MinMaxScaler()
df_transformed_Power = pd.DataFrame(scale_MinMax.fit_transform(df_power), columns=columns_names)
scale_Standard = StandardScaler()
df_transformed_Power = pd.DataFrame(scale_Standard.fit_transform(df_power), columns=columns_names)
df_log.to_csv("./Data_Log.csv",index=False)
df_transformed = pd.read_csv('/home/tpriya/CS5525/MLBlog/posts/outlier-detection/Data_Log.csv')
df_transformed
# To plot Elbow With Inertia
inertia = []
Range = [*range(1,11)]
for k in Range:
kmean = KMeans(n_clusters=k, max_iter=300, random_state=42)
kmean.fit(df_transformed)
inertia.append(kmean.inertia_)
plt.figure(figsize=(10,4))
plt.plot(Range, inertia, 'bx-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('The Elbow Method ')
plt.show()
drop_variation = []
drop_variation.append(0) #add 0 in the first element
for i in range(len(inertia) -1):
dropValue = inertia[i] - inertia[i+1]
drop_variation.append(dropValue)
# select suitable k that have large drop in the variation
k = Range[np.argmax(drop_variation)]
print("Suitable number of clusters = ",k)
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, random_state=42).fit(df_transformed)
labels = kmeans.labels_
iner = kmeans.inertia_
cent = kmeans.cluster_centers_
print("\t~~ THIS RESULT OF K-mean SKLEARN  ~~")
print('~'*50)
print("sum of elements that contain in cluster 0 :",(labels == 0).sum())
print("sum of elements that contain in cluster 1 :",(labels == 1).sum())
print("sum of elements that contain in cluster 2 :",(labels == 2).sum())
print("sum of elements that contain in cluster 3 :",(labels == 3).sum())
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.metrics import silhouette_score
score = silhouette_score(df_transformed,  labels, metric='euclidean')
print('Silhouett Score: %.3f' % score)
visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')
visualizer.fit(df_transformed)
visualizer.show()
from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components=10, kernel='rbf')
df_kpca = pd.DataFrame(kpca.fit_transform(df_transformed))
df_kpca
from sklearn.ensemble import IsolationForest
clf = IsolationForest(random_state=0,
max_features=2,
n_estimators=100,
contamination=0.1).fit(df_kpca)
anom_pred = clf.predict(df_kpca)
anom_pred
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2,
perplexity=50,
random_state=42,
n_iter=300).fit_transform(df_transformed)
df_embed_Iso = pd.DataFrame(tsne, columns=['feature1', 'feature2'])
df_embed_Iso['Labels']= pd.DataFrame(anom_pred)
df_embed_Iso
plt.figure(figsize=(9,5))
sns.scatterplot(
x='feature1', y='feature2',
data=df_embed_Iso,
hue=df_embed_Iso['Labels'],
palette=sns.color_palette("hls", 2)
)
